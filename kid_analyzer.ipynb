{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Guida all'Analisi Automatica dei Documenti KID\n",
        "\n",
        "Questo notebook √® stato sviluppato per automatizzare il processo di acquisizione, trasformazione ed estrazione di dati da documenti in formato \"Key Information Document\" (KID) o altri documenti PDF simili. L'obiettivo √® facilitare l'analisi e la comprensione delle informazioni contenute in questi documenti.\n",
        "\n",
        "## Panoramica del Flusso Operativo:\n",
        "\n",
        "Il processo si articola in diverse fasi principali, come evidenziato nelle sezioni successive di questo notebook:\n",
        "\n",
        "1.  **Installazione delle Dipendenze**: Vengono installate le librerie Python necessarie per la manipolazione dei PDF e la loro conversione. Nello specifico, si installa `pdf-to-markdown` e le sue dipendenze, come `pdfminer.six`, essenziali per convertire i PDF in un formato testuale pi√π gestibile [2].\n",
        "\n",
        "2.  **Scarico dei PDF**: Il notebook procede al download dei documenti PDF da un elenco di URL. I log mostrano che vengono trovati e scaricati numerosi file PDF, con alcuni tentativi di recupero in caso di timeout o errori di accesso (es. \"Access denied\" o \"File not found\") [3-5]. I file scaricati vengono salvati in una directory locale [4, 5].\n",
        "\n",
        "3.  **Conversione da PDF a Markdown**: Dopo aver scaricato i PDF, il notebook avvia un processo per convertirli nel formato Markdown. Questo passaggio √® cruciale per standardizzare il contenuto e renderlo facilmente processabile da modelli di linguaggio [6, 7]. I file Markdown risultanti vengono caricati e la loro dimensione (in caratteri) viene registrata [7].\n",
        "\n",
        "4.  **Generazione e Lancio dei Prompt (Analisi Intelligente)**: Vengono generati dei \"prompt intelligenti\" basati sul contenuto dei file Markdown. Questi prompt vengono poi utilizzati per interrogare un modello di linguaggio (probabilmente GPT, dato il riferimento a \"Processing KID documents with GPT\") per estrarre informazioni strutturate dai documenti. Il processo monitora l'avanzamento, il numero di KID estratti e segnala eventuali risposte troncate o problemi nel formato JSON [7-9].\n",
        "\n",
        "5.  **Estrazione e Pulizia dei Dati in Pandas**: Le informazioni estratte dal modello di linguaggio vengono caricate e organizzate in un DataFrame Pandas [9]. Successivamente, viene eseguito un processo di \"pulizia del dato\" e uniformazione avanzata del dataset per standardizzare le colonne e i valori, preparandoli per ulteriori analisi [9-11].\n",
        "\n",
        "Questo flusso permette di trasformare una collezione di documenti PDF non strutturati in un dataset pulito e analizzabile, pronto per essere utilizzato per scopi di ricerca, comparazione o reportistica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installazione delle Dipendenze Essenziali\n",
        "\n",
        "Questa sezione del notebook si occupa di configurare l'ambiente installando tutte le librerie Python necessarie per l'analisi dei documenti. √à un passaggio fondamentale per assicurare che tutte le funzionalit√† successive possano operare correttamente.\n",
        "\n",
        "### Scopo:\n",
        "\n",
        "Lo scopo principale √® fornire al notebook gli strumenti software per:\n",
        "*   Interagire con i file PDF.\n",
        "*   Convertire il contenuto dei PDF in un formato testuale leggibile e processabile.\n",
        "\n",
        "### Dettagli Tecnici:\n",
        "\n",
        "Come puoi vedere dall'output sottostante, il processo installa le seguenti librerie chiave [1]:\n",
        "\n",
        "*   **`pdf-to-markdown`**: Questa √® la libreria principale per convertire i documenti PDF nel formato Markdown. √à cruciale per trasformare il contenuto non strutturato dei PDF in un formato pi√π standardizzato, facilitando l'estrazione di informazioni.\n",
        "*   **`pdfminer.six`**: √à una dipendenza di `pdf-to-markdown` e costituisce un'infrastruttura robusta per l'estrazione di testo e altre informazioni dai documenti PDF [1].\n",
        "*   Altre dipendenze come `charset-normalizer`, `cryptography`, `cffi`, e `pycparser` sono automaticamente gestite e installate (o verificate se gi√† presenti) per supportare il funzionamento di `pdfminer.six` [1].\n",
        "\n",
        "L'output mostra il download e la preparazione dei metadati, la risoluzione delle dipendenze e, infine, la conferma dell'avvenuta installazione di `pdf-to-markdown` e `pdfminer.six` [1]. Questo assicura che il notebook sia pronto per procedere con le fasi successive di scaricamento e conversione dei documenti."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhcOVK61AO3J",
        "outputId": "79b6ca6c-3339-4262-c571-c23afea1a7b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pdf-to-markdown\n",
            "  Downloading pdf-to-markdown-0.1.0.tar.gz (6.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfminer.six (from pdf-to-markdown)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six->pdf-to-markdown) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six->pdf-to-markdown) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six->pdf-to-markdown) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pdf-to-markdown) (2.22)\n",
            "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pdf-to-markdown\n",
            "  Building wheel for pdf-to-markdown (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdf-to-markdown: filename=pdf_to_markdown-0.1.0-py3-none-any.whl size=8139 sha256=bbdd6d80fd1a09c9ae7c8fac64e816d1b04afd7371fb0fdf2549db20de7f3f72\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/dd/fe/f94c16513eef3f4f229e16a9c88c3e14f6f38b400d24f3b631\n",
            "Successfully built pdf-to-markdown\n",
            "Installing collected packages: pdfminer.six, pdf-to-markdown\n",
            "Successfully installed pdf-to-markdown-0.1.0 pdfminer.six-20250506\n"
          ]
        }
      ],
      "source": [
        "pip install pdf-to-markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgdF6owXzbBr"
      },
      "source": [
        "## 2. Scaricamento dei Documenti PDF\n",
        "\n",
        "Questa sezione si occupa del reperimento dei documenti \"Key Information Document\" (KID) o altri file PDF rilevanti da una lista di URL predefinita (indicata come \"CSV\" nei log [2]). √à un passo critico per raccogliere i dati grezzi che verranno successivamente analizzati.\n",
        "\n",
        "### Scopo:\n",
        "\n",
        "L'obiettivo di questa fase √® scaricare in modo automatizzato tutti i documenti PDF necessari per l'analisi, creando una collezione locale di file da processare.\n",
        "\n",
        "### Dettagli del Processo:\n",
        "\n",
        "Il notebook esegue le seguenti operazioni, come evidenziato dagli output [2, 3]:\n",
        "\n",
        "*   **Identificazione dei Record**: Il processo inizia con la rilevazione di un certo numero di record (es. \"Found 60 records in CSV\" [2]), presumibilmente corrispondenti a URL di documenti.\n",
        "*   **Tentativi di Download**: Per ogni URL, il notebook tenta di scaricare il file PDF. I log mostrano un \"attempt 1\" (tentativo 1) per ogni download [2, 3].\n",
        "*   **Gestione degli Errori**: Il sistema √® progettato per gestire diverse casistiche durante lo scaricamento:\n",
        "    *   **Download Riuscito (`‚úì Saved`)**: Quando un file viene scaricato correttamente, viene visualizzato un messaggio di successo e il percorso in cui √® stato salvato, ad esempio `‚úì Saved: downloaded_pdfs/A>>>>>a_Assicurazioni_S.p.A._Partner_M>>>>_Opportunity.pdf` [2].\n",
        "    *   **Accesso Negato (`‚ùå Access denied (403)`)**: Questo errore si verifica quando il server rifiuta la richiesta di accesso al file, spesso a causa di permessi insufficienti. Un esempio √® `‚ùå Access denied (403) for https://www.c>>vita.it/documenti/...` [2].\n",
        "    *   **File Non Trovato (`‚ùå File not found (404)`)**: Indica che l'URL √® valido ma il file specifico non esiste sul server. Un esempio √® `‚ùå File not found (404) for https://www.z>>>>>.it/api/ArchivioDigitale/...` [2].\n",
        "    *   **Timeout**: Si verifica quando la connessione al server impiega troppo tempo per rispondere o completare il download. Il notebook tenta di riprovare il download dopo un breve intervallo (`Retrying in 1 seconds...`, `Retrying in 2 seconds...`) come mostrato per i file `n>>>>>>>>.it` [2].\n",
        "\n",
        "Questa fase √® fondamentale per costruire il corpus di documenti su cui verr√† eseguita l'analisi, e la gestione degli errori √® cruciale per identificare quali documenti non sono stati acquisiti e perch√©."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYXK01t8w7oc",
        "outputId": "265ab228-8736-4895-8322-c8f56c84aacb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "import time\n",
        "\n",
        "# Assicurati che il file CSV sia stato caricato nell'ambiente di Colab\n",
        "csv_file_path = 'kid_compagnie_vita_italiane.csv'\n",
        "\n",
        "def clean_filename(filename):\n",
        "    \"\"\"Pulisce il nome del file rimuovendo caratteri non validi\"\"\"\n",
        "    # Rimuovi caratteri non validi per i nomi file\n",
        "    filename = re.sub(r'[<>:\"/\\\\|?*()[\\]]', '_', filename)\n",
        "    # Rimuovi spazi multipli e sostituisci con underscore\n",
        "    filename = re.sub(r'\\s+', '_', filename)\n",
        "    # Rimuovi punti multipli\n",
        "    filename = re.sub(r'\\.+', '.', filename)\n",
        "    # Limita la lunghezza del nome file\n",
        "    if len(filename) > 200:\n",
        "        filename = filename[:200]\n",
        "    return filename\n",
        "\n",
        "def download_with_retry(url, file_path, max_retries=3, delay=1):\n",
        "    \"\"\"Scarica un file con retry automatico\"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Accept': 'application/pdf,application/octet-stream,*/*',\n",
        "        'Accept-Language': 'it-IT,it;q=0.9,en;q=0.8',\n",
        "        'Accept-Encoding': 'gzip, deflate, br',\n",
        "        'Connection': 'keep-alive',\n",
        "        'Upgrade-Insecure-Requests': '1'\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"Downloading (attempt {attempt + 1}): {url}\")\n",
        "\n",
        "            # Timeout pi√π lungo e headers per evitare blocchi\n",
        "            response = requests.get(url, stream=True, headers=headers, timeout=30)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Verifica che sia effettivamente un PDF\n",
        "            content_type = response.headers.get('content-type', '')\n",
        "            if 'pdf' not in content_type.lower() and 'octet-stream' not in content_type.lower():\n",
        "                print(f\"Warning: Content type is {content_type}, might not be a PDF\")\n",
        "\n",
        "            # Salva il PDF\n",
        "            with open(file_path, 'wb') as pdf_file:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    if chunk:  # Filtra i chunk vuoti\n",
        "                        pdf_file.write(chunk)\n",
        "\n",
        "            print(f\"‚úì Saved: {file_path}\")\n",
        "            return True\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"Timeout for {url} (attempt {attempt + 1})\")\n",
        "        except requests.exceptions.ConnectionError as e:\n",
        "            print(f\"Connection error for {url}: {e} (attempt {attempt + 1})\")\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            if response.status_code == 403:\n",
        "                print(f\"‚ùå Access denied (403) for {url}\")\n",
        "                return False\n",
        "            elif response.status_code == 404:\n",
        "                print(f\"‚ùå File not found (404) for {url}\")\n",
        "                return False\n",
        "            else:\n",
        "                print(f\"HTTP error {response.status_code} for {url} (attempt {attempt + 1})\")\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error for {url}: {e} (attempt {attempt + 1})\")\n",
        "\n",
        "        if attempt < max_retries - 1:\n",
        "            print(f\"Retrying in {delay} seconds...\")\n",
        "            time.sleep(delay)\n",
        "            delay *= 2  # Aumenta il delay per ogni retry\n",
        "\n",
        "    return False\n",
        "\n",
        "try:\n",
        "    # Leggi il file CSV\n",
        "    df = pd.read_csv(csv_file_path, delimiter=';')\n",
        "    print(f\"Found {len(df)} records in CSV\")\n",
        "\n",
        "    # Crea una directory per salvare i PDF (se non esiste)\n",
        "    output_dir = 'downloaded_pdfs'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Statistiche\n",
        "    successful_downloads = 0\n",
        "    failed_downloads = 0\n",
        "    skipped_downloads = 0\n",
        "\n",
        "    # Itera sulle righe del DataFrame e scarica i PDF\n",
        "    for index, row in df.iterrows():\n",
        "        compagnia = row['Compagnia']\n",
        "        prodotto = row['Prodotto']\n",
        "        link_kid = row['Link_KID']\n",
        "\n",
        "        # Verifica che il link non sia vuoto\n",
        "        if pd.isna(link_kid) or not link_kid.strip():\n",
        "            print(f\"‚ö†Ô∏è Skipping empty link for {compagnia} - {prodotto}\")\n",
        "            skipped_downloads += 1\n",
        "            continue\n",
        "\n",
        "        # Pulisci i nomi per usarli nei nomi dei file\n",
        "        clean_compagnia = clean_filename(compagnia)\n",
        "        clean_prodotto = clean_filename(prodotto)\n",
        "        file_name = f\"{clean_compagnia}_{clean_prodotto}.pdf\"\n",
        "        file_path = os.path.join(output_dir, file_name)\n",
        "\n",
        "        # Controlla se il file esiste gi√†\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"File already exists, skipping: {file_path}\")\n",
        "            skipped_downloads += 1\n",
        "            continue\n",
        "\n",
        "        # Scarica il PDF\n",
        "        if download_with_retry(link_kid, file_path):\n",
        "            successful_downloads += 1\n",
        "        else:\n",
        "            failed_downloads += 1\n",
        "\n",
        "        # Piccola pausa tra i download per essere gentili con i server\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    # Statistiche finali\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"DOWNLOAD SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"‚úì Successful downloads: {successful_downloads}\")\n",
        "    print(f\"‚ùå Failed downloads: {failed_downloads}\")\n",
        "    print(f\"‚ö†Ô∏è Skipped downloads: {skipped_downloads}\")\n",
        "    print(f\"üìä Total processed: {len(df)}\")\n",
        "    print(f\"üìÅ Files saved in: {output_dir}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: The file '{csv_file_path}' was not found. Please upload it to your Colab environment.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå An error occurred while reading the CSV: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6O_H9jT5xP0"
      },
      "source": [
        "## 3. Conversione da PDF a Markdown\n",
        "\n",
        "Dopo aver scaricato i documenti PDF, questa sezione del notebook si dedica alla loro trasformazione in un formato pi√π gestibile e standardizzato: il Markdown. Questo passaggio √® fondamentale per preparare i testi all'analisi automatizzata.\n",
        "\n",
        "### Scopo:\n",
        "\n",
        "L'obiettivo principale di questa fase √® convertire il contenuto non strutturato dei documenti PDF in testo formattato in Markdown [1]. Il formato Markdown facilita l'estrazione di informazioni e la successiva elaborazione da parte di modelli di linguaggio, rendendo i documenti \"leggibili\" per gli algoritmi [1].\n",
        "\n",
        "### Dettagli del Processo:\n",
        "\n",
        "Come indicato dagli output, il processo di conversione include i seguenti passaggi:\n",
        "\n",
        "*   **Avvio dell'Elaborazione**: Il notebook avvia l'elaborazione dei documenti PDF [2].\n",
        "*   **Rilevamento dei File**: Viene riportato il numero di file PDF trovati e pronti per essere processati (ad esempio, \"Trovati 67 file PDF da processare\") [2].\n",
        "*   **Conversione e Caricamento**: Ogni file PDF scaricato viene convertito in un file Markdown corrispondente. Successivamente, questi file Markdown vengono caricati, e il loro contenuto viene misurato in termini di numero di caratteri (ad esempio, \"Loaded /content/markdown_output/...md: XXXXX characters\") [3]. Questo controllo sulla dimensione dei file Markdown indica che la conversione √® avvenuta con successo e che il contenuto testuale √® stato acquisito [3].\n",
        "\n",
        "Questa fase √® cruciale per la normalizzazione del formato dei documenti, trasformando una collezione eterogenea di PDF in un corpus di testo uniforme pronto per l'analisi intelligente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLtFoL-L3a6g",
        "outputId": "569d16f8-cc43-476c-a782-15fd20975be5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import io\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "def pdf_to_markdown_with_ocr(pdf_path, lang='ita+eng'):\n",
        "    \"\"\"\n",
        "    Converte un PDF in markdown estraendo testo normale e facendo OCR sulle immagini\n",
        "\n",
        "    Args:\n",
        "        pdf_path: percorso del file PDF\n",
        "        lang: lingue per OCR (default: italiano + inglese)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (markdown_text, extraction_stats)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        markdown_text = \"\"\n",
        "        stats = {\n",
        "            'pages': len(doc),\n",
        "            'text_pages': 0,\n",
        "            'images_processed': 0,\n",
        "            'ocr_text_found': 0,\n",
        "            'errors': []\n",
        "        }\n",
        "\n",
        "        markdown_text += f\"# {os.path.basename(pdf_path)}\\n\\n\"\n",
        "        markdown_text += f\"*Estratto il: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\\n\\n\"\n",
        "\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "\n",
        "            # Estrai testo normale\n",
        "            text = page.get_text()\n",
        "            markdown_text += f\"## Pagina {page_num + 1}\\n\\n\"\n",
        "\n",
        "            if text.strip():\n",
        "                stats['text_pages'] += 1\n",
        "                # Pulisci il testo (rimuovi righe vuote eccessive)\n",
        "                cleaned_text = '\\n'.join(line for line in text.split('\\n') if line.strip())\n",
        "                markdown_text += cleaned_text + \"\\n\\n\"\n",
        "\n",
        "            # Estrai e processa immagini\n",
        "            image_list = page.get_images()\n",
        "\n",
        "            if image_list:\n",
        "                markdown_text += f\"### Immagini trovate: {len(image_list)}\\n\\n\"\n",
        "\n",
        "            for img_index, img in enumerate(image_list):\n",
        "                try:\n",
        "                    # Estrai l'immagine\n",
        "                    xref = img[0]\n",
        "                    pix = fitz.Pixmap(doc, xref)\n",
        "\n",
        "                    if pix.n - pix.alpha < 4:  # GRAY o RGB\n",
        "                        stats['images_processed'] += 1\n",
        "                        img_data = pix.tobytes(\"png\")\n",
        "                        img_pil = Image.open(io.BytesIO(img_data))\n",
        "\n",
        "                        # OCR sull'immagine\n",
        "                        ocr_text = pytesseract.image_to_string(img_pil, lang=lang)\n",
        "\n",
        "                        if ocr_text.strip():\n",
        "                            stats['ocr_text_found'] += 1\n",
        "                            markdown_text += f\"#### Testo estratto da immagine {img_index + 1}:\\n\"\n",
        "                            markdown_text += f\"```\\n{ocr_text.strip()}\\n```\\n\\n\"\n",
        "\n",
        "                    pix = None\n",
        "\n",
        "                except Exception as e:\n",
        "                    error_msg = f\"Errore nel processare immagine {img_index + 1} a pagina {page_num + 1}: {str(e)}\"\n",
        "                    stats['errors'].append(error_msg)\n",
        "                    print(f\"‚ö†Ô∏è {error_msg}\")\n",
        "\n",
        "        doc.close()\n",
        "        return markdown_text, stats\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Errore nel processare {pdf_path}: {str(e)}\"\n",
        "        return f\"# ERRORE\\n\\nImpossibile processare il file: {error_msg}\\n\", {'errors': [error_msg]}\n",
        "\n",
        "def process_all_pdfs(pdf_directory='downloaded_pdfs', output_directory='markdown_output'):\n",
        "    \"\"\"\n",
        "    Processa tutti i PDF in una directory e salva i risultati in markdown\n",
        "\n",
        "    Args:\n",
        "        pdf_directory: directory contenente i PDF\n",
        "        output_directory: directory dove salvare i file markdown\n",
        "    \"\"\"\n",
        "\n",
        "    # Installa le dipendenze necessarie\n",
        "    print(\"üîß Installazione dipendenze...\")\n",
        "    os.system('pip install pymupdf pillow pytesseract -q')\n",
        "    os.system('apt install tesseract-ocr tesseract-ocr-ita -y -q')\n",
        "\n",
        "    # Crea directory di output\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "    # Trova tutti i PDF\n",
        "    if not os.path.exists(pdf_directory):\n",
        "        print(f\"‚ùå Directory {pdf_directory} non trovata!\")\n",
        "        return\n",
        "\n",
        "    pdf_files = [f for f in os.listdir(pdf_directory) if f.lower().endswith('.pdf')]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(f\"‚ùå Nessun PDF trovato in {pdf_directory}\")\n",
        "        return\n",
        "\n",
        "    print(f\"üìÅ Trovati {len(pdf_files)} file PDF da processare\")\n",
        "\n",
        "    # Statistiche globali\n",
        "    global_stats = {\n",
        "        'total_files': len(pdf_files),\n",
        "        'successful': 0,\n",
        "        'failed': 0,\n",
        "        'total_pages': 0,\n",
        "        'total_text_pages': 0,\n",
        "        'total_images': 0,\n",
        "        'total_ocr_extractions': 0,\n",
        "        'processing_times': []\n",
        "    }\n",
        "\n",
        "    # Salva anche un file di log dettagliato\n",
        "    log_data = []\n",
        "\n",
        "    # Processa ogni PDF\n",
        "    for i, pdf_file in enumerate(pdf_files, 1):\n",
        "        pdf_path = os.path.join(pdf_directory, pdf_file)\n",
        "        output_filename = pdf_file.replace('.pdf', '.md')\n",
        "        output_path = os.path.join(output_directory, output_filename)\n",
        "\n",
        "        print(f\"\\nüìÑ [{i}/{len(pdf_files)}] Processando: {pdf_file}\")\n",
        "\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        try:\n",
        "            # Converti PDF in markdown\n",
        "            markdown_content, stats = pdf_to_markdown_with_ocr(pdf_path)\n",
        "\n",
        "            # Salva il file markdown\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(markdown_content)\n",
        "\n",
        "            processing_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "            # Aggiorna statistiche\n",
        "            global_stats['successful'] += 1\n",
        "            global_stats['total_pages'] += stats.get('pages', 0)\n",
        "            global_stats['total_text_pages'] += stats.get('text_pages', 0)\n",
        "            global_stats['total_images'] += stats.get('images_processed', 0)\n",
        "            global_stats['total_ocr_extractions'] += stats.get('ocr_text_found', 0)\n",
        "            global_stats['processing_times'].append(processing_time)\n",
        "\n",
        "            # Log dettagliato\n",
        "            log_entry = {\n",
        "                'file': pdf_file,\n",
        "                'status': 'success',\n",
        "                'processing_time': processing_time,\n",
        "                'stats': stats,\n",
        "                'output_file': output_filename\n",
        "            }\n",
        "            log_data.append(log_entry)\n",
        "\n",
        "            print(f\"‚úÖ Completato in {processing_time:.1f}s - {stats.get('pages', 0)} pagine, {stats.get('images_processed', 0)} immagini\")\n",
        "\n",
        "        except Exception as e:\n",
        "            global_stats['failed'] += 1\n",
        "            error_msg = str(e)\n",
        "\n",
        "            log_entry = {\n",
        "                'file': pdf_file,\n",
        "                'status': 'failed',\n",
        "                'error': error_msg,\n",
        "                'processing_time': (datetime.now() - start_time).total_seconds()\n",
        "            }\n",
        "            log_data.append(log_entry)\n",
        "\n",
        "            print(f\"‚ùå Errore: {error_msg}\")\n",
        "\n",
        "    # Salva log dettagliato\n",
        "    log_path = os.path.join(output_directory, 'processing_log.json')\n",
        "    with open(log_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(log_data, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "    # Crea report riassuntivo\n",
        "    create_summary_report(global_stats, log_data, output_directory)\n",
        "\n",
        "    # Stampa statistiche finali\n",
        "    print_final_statistics(global_stats, output_directory)\n",
        "\n",
        "def create_summary_report(stats, log_data, output_dir):\n",
        "    \"\"\"Crea un report riassuntivo in markdown\"\"\"\n",
        "\n",
        "    avg_time = sum(stats['processing_times']) / len(stats['processing_times']) if stats['processing_times'] else 0\n",
        "\n",
        "    report = f\"\"\"# Report di Elaborazione PDF\n",
        "\n",
        "*Generato il: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
        "\n",
        "## Statistiche Generali\n",
        "\n",
        "- **File totali processati:** {stats['total_files']}\n",
        "- **Successi:** {stats['successful']} ‚úÖ\n",
        "- **Fallimenti:** {stats['failed']} ‚ùå\n",
        "- **Tasso di successo:** {(stats['successful']/stats['total_files']*100):.1f}%\n",
        "\n",
        "## Statistiche di Contenuto\n",
        "\n",
        "- **Pagine totali elaborate:** {stats['total_pages']}\n",
        "- **Pagine con testo:** {stats['total_text_pages']}\n",
        "- **Immagini processate:** {stats['total_images']}\n",
        "- **Estrazioni OCR riuscite:** {stats['total_ocr_extractions']}\n",
        "\n",
        "## Performance\n",
        "\n",
        "- **Tempo medio per file:** {avg_time:.1f} secondi\n",
        "- **Tempo totale:** {sum(stats['processing_times']):.1f} secondi\n",
        "\n",
        "## Dettagli per File\n",
        "\n",
        "| File | Status | Tempo (s) | Pagine | Immagini | OCR |\n",
        "|------|--------|-----------|---------|----------|-----|\n",
        "\"\"\"\n",
        "\n",
        "    for entry in log_data:\n",
        "        status_icon = \"‚úÖ\" if entry['status'] == 'success' else \"‚ùå\"\n",
        "        time_val = f\"{entry['processing_time']:.1f}\"\n",
        "\n",
        "        if entry['status'] == 'success':\n",
        "            pages = entry['stats'].get('pages', 0)\n",
        "            images = entry['stats'].get('images_processed', 0)\n",
        "            ocr = entry['stats'].get('ocr_text_found', 0)\n",
        "            report += f\"| {entry['file']} | {status_icon} | {time_val} | {pages} | {images} | {ocr} |\\n\"\n",
        "        else:\n",
        "            report += f\"| {entry['file']} | {status_icon} | {time_val} | - | - | - |\\n\"\n",
        "\n",
        "    # Salva il report\n",
        "    report_path = os.path.join(output_dir, 'REPORT_ELABORAZIONE.md')\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(report)\n",
        "\n",
        "def print_final_statistics(stats, output_dir):\n",
        "    \"\"\"Stampa le statistiche finali\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéâ ELABORAZIONE COMPLETATA!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"üìä File processati: {stats['successful']}/{stats['total_files']}\")\n",
        "    print(f\"üìÑ Pagine elaborate: {stats['total_pages']}\")\n",
        "    print(f\"üñºÔ∏è Immagini processate: {stats['total_images']}\")\n",
        "    print(f\"üîç Estrazioni OCR: {stats['total_ocr_extractions']}\")\n",
        "\n",
        "    if stats['processing_times']:\n",
        "        avg_time = sum(stats['processing_times']) / len(stats['processing_times'])\n",
        "        print(f\"‚è±Ô∏è Tempo medio: {avg_time:.1f}s per file\")\n",
        "\n",
        "    print(f\"üìÅ File salvati in: {output_dir}\")\n",
        "    print(f\"üìã Log dettagliato: {output_dir}/processing_log.json\")\n",
        "    print(f\"üìÑ Report completo: {output_dir}/REPORT_ELABORAZIONE.md\")\n",
        "\n",
        "    if stats['failed'] > 0:\n",
        "        print(f\"‚ö†Ô∏è {stats['failed']} file non sono stati processati correttamente\")\n",
        "\n",
        "# Esegui il processamento\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ Avvio elaborazione PDF...\")\n",
        "    process_all_pdfs()\n",
        "    print(\"\\n‚ú® Processo completato!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_Hd2-6_9YrN"
      },
      "source": [
        "## 4. Generazione e Lancio dei Prompt per l'Analisi Intelligente\n",
        "\n",
        "Questa sezione rappresenta il cuore dell'analisi automatizzata, dove il contenuto testuale estratto dai documenti Markdown viene utilizzato per interrogare un modello di linguaggio avanzato, al fine di estrarre informazioni strutturate e rilevanti.\n",
        "\n",
        "### Scopo:\n",
        "\n",
        "L'obiettivo primario di questa fase √® trasformare il testo non strutturato dei documenti in dati utilizzabili, attraverso l'interazione con un modello di Intelligenza Artificiale. Il processo √® progettato per identificare e raccogliere automaticamente le informazioni chiave dai documenti, come indicato nel formato \"Key Information Document\" (KID) o simili.\n",
        "\n",
        "### Dettagli del Processo:\n",
        "\n",
        "Come evidenziato dagli output di questa sezione, il flusso di lavoro comprende:\n",
        "\n",
        "*   **Preparazione dei Prompt**: Vengono creati dei \"prompt intelligenti\" (ad esempio, \"Generated 68 intelligent analysis prompts for 68 documents\") basati sul contenuto dei file Markdown precedentemente generati. Questi prompt sono formulati per guidare il modello di linguaggio nell'estrazione delle informazioni desiderate.\n",
        "*   **Avvio dell'Estrazione**: Il processo di estrazione delle informazioni viene avviato, con un'indicazione chiara che si tratta di \"Processing KID documents with GPT...\" [1]. Questo suggerisce l'utilizzo di un modello avanzato per l'elaborazione.\n",
        "*   **Monitoraggio dell'Avanzamento**: Durante l'esecuzione, il notebook fornisce aggiornamenti sullo stato di avanzamento (es. \"Progress: 10/68\", \"Progress: 20/68\", ecc.) [2]. Questo permette di tenere traccia di quanti documenti sono stati processati.\n",
        "*   **Validazione e Stato delle Risposte**: Il sistema monitora la qualit√† delle risposte ottenute dal modello di linguaggio, classificandole in:\n",
        "    *   `Valid JSON`: Risposte correttamente formattate in JSON e completamente elaborate [2].\n",
        "    *   `Partial`: Risposte JSON parziali, che potrebbero indicare problemi nell'estrazione completa dei dati [2].\n",
        "    *   `Invalid`: Risposte che non sono conformi al formato JSON atteso [2].\n",
        "    *   Viene anche segnalato il numero di `Truncated responses` (risposte troncate), il che indica che il modello potrebbe aver raggiunto il limite di token nella sua risposta, impedendo un'estrazione completa [2].\n",
        "*   **Conteggio delle Informazioni Estratte**: Il notebook traccia il `Total KIDs extracted` (numero totale di informazioni chiave estratte) [2], fornendo una metrica cumulativa del successo dell'estrazione.\n",
        "\n",
        "Questa fase √® cruciale per la trasformazione dei dati grezzi in un formato strutturato e pronto per l'analisi successiva, anche se occasionalmente si possono riscontrare limitazioni o problemi nelle risposte del modello."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Prima provvediamo a mettere al sicuro i markdown che abbiamo generato*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!sudo apt install p7zip-full p7zip-rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uk-tfaE4H9pw",
        "outputId": "4728243e-63cd-4600-fbae-4917978b450d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 564451 bytes (552 KiB)\n",
            "\n",
            "Extracting archive: mds.7z\n",
            "--\n",
            "Path = mds.7z\n",
            "Type = 7z\n",
            "Physical Size = 564451\n",
            "Headers Size = 2062\n",
            "Method = LZMA2:23\n",
            "Solid = +\n",
            "Blocks = 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\bEverything is Ok\n",
            "\n",
            "Files: 69\n",
            "Size:       6849850\n",
            "Compressed: 564451\n"
          ]
        }
      ],
      "source": [
        "!7z x mds.7z -o/markdown_output/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfEK3zK7IXh5"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/markdown_output/\n",
        "!mv /markdown_output/markdown_output/*.md /content/markdown_output/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HWHYDlyy9bg4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "def generate_kid_analysis_prompts(kid_documents, output_file='prompts_kid_analysis.txt'):\n",
        "    \"\"\"\n",
        "    Genera prompt per GPT-4.1 per identificare automaticamente e estrarre informazioni\n",
        "    da uno o pi√π KID contenuti in un documento markdown. GPT gestisce autonomamente\n",
        "    la rilevazione e separazione dei KID multipli.\n",
        "\n",
        "    Args:\n",
        "        kid_documents (list): Lista di documenti markdown che possono contenere uno o pi√π KID\n",
        "        output_file (str): Nome del file in cui salvare i prompt generati\n",
        "\n",
        "    Returns:\n",
        "        list: Lista dei prompt generati in formato JSON\n",
        "    \"\"\"\n",
        "\n",
        "    # System prompt che delega tutto a GPT\n",
        "    system_prompt = \"\"\"You are an expert financial analyst specializing in KID (Key Information Documents) analysis for insurance and investment products. Your task is to:\n",
        "\n",
        "1. AUTOMATICALLY DETECT how many individual KID documents are contained within the provided markdown content\n",
        "2. EXTRACT comprehensive structured data for EACH individual KID found\n",
        "3. GENERATE descriptive texts for each KID\n",
        "4. RETURN a JSON response that reflects the actual number of KIDs found\n",
        "\n",
        "You must analyze the entire document and identify distinct KID sections based on headers like:\n",
        "- \"DOCUMENTO CONTENENTE LE INFORMAZIONI CHIAVE\"\n",
        "- \"DOCUMENTO CONTENENTE LE INFORMAZIONI SPECIFICHE\"\n",
        "- Product name changes (e.g., different investment options)\n",
        "- Clear section breaks between different financial products\n",
        "\n",
        "For each KID found, extract complete information including numerical data, risk metrics, cost structures, target market, and generate engaging descriptive content. Return everything in a structured JSON format that clearly shows how many individual KIDs were processed.\"\"\"\n",
        "\n",
        "    prompts = []\n",
        "\n",
        "    for i, kid_doc in enumerate(kid_documents):\n",
        "        # Gestione input (stringa o dict)\n",
        "        if isinstance(kid_doc, str):\n",
        "            doc_content = kid_doc\n",
        "            doc_id = f\"DOC_{i+1}\"\n",
        "        elif isinstance(kid_doc, dict):\n",
        "            doc_content = kid_doc.get('content', '')\n",
        "            doc_id = kid_doc.get('id', f\"DOC_{i+1}\")\n",
        "        else:\n",
        "            print(f\"Warning: Invalid document format for document {i+1}\")\n",
        "            continue\n",
        "\n",
        "        if not doc_content.strip():\n",
        "            print(f\"Warning: Empty content for document {doc_id}\")\n",
        "            continue\n",
        "\n",
        "        # User prompt che chiede a GPT di gestire tutto\n",
        "        user_prompt = f\"\"\"Analyze the following markdown document and automatically identify how many individual KID (Key Information Documents) sections it contains. Then extract comprehensive information for each KID found.\n",
        "\n",
        "DOCUMENT ID: \"{doc_id}\"\n",
        "DOCUMENT CONTENT:\n",
        "\"{doc_content}\"\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. First, identify how many distinct KID documents are present in this content\n",
        "2. For each KID found, extract all information according to the structure below\n",
        "3. Return a JSON with an array containing one object per KID identified\n",
        "\n",
        "REQUIRED JSON STRUCTURE:\n",
        "{{\n",
        "  \"document_analysis\": {{\n",
        "    \"source_document_id\": \"{doc_id}\",\n",
        "    \"analysis_date\": \"{datetime.now().isoformat()}\",\n",
        "    \"total_kids_found\": 0,\n",
        "    \"processing_notes\": \"Brief description of how many KIDs were identified and any challenges\"\n",
        "  }},\n",
        "  \"kids\": [\n",
        "    {{\n",
        "      \"kid_id\": \"auto_generated_unique_id\",\n",
        "      \"kid_sequence\": 1,\n",
        "      \"product_identification\": {{\n",
        "        \"nome_prodotto\": \"\",\n",
        "        \"compagnia\": \"\",\n",
        "        \"data_documento\": \"\",\n",
        "        \"tipo_prodotto\": \"\",\n",
        "        \"durata_contratto\": \"\",\n",
        "        \"periodo_versamenti\": \"\"\n",
        "      }},\n",
        "      \"investment_structure\": {{\n",
        "        \"opzioni_investimento\": [\n",
        "          {{\n",
        "            \"nome_opzione\": \"\",\n",
        "            \"allocazione_percentuale\": \"\",\n",
        "            \"tipologia\": \"gestione_separata | fondo_interno\",\n",
        "            \"obiettivo_investimento\": \"\",\n",
        "            \"livello_rischio\": \"1-7\",\n",
        "            \"tematiche_esg\": []\n",
        "          }}\n",
        "        ]\n",
        "      }},\n",
        "      \"risk_return_profile\": {{\n",
        "        \"indicatore_rischio_sintetico\": \"1-7\",\n",
        "        \"periodo_detenzione_raccomandato\": \"\",\n",
        "        \"periodo_minimo\": \"\",\n",
        "        \"protezione_capitale\": \"si/no\",\n",
        "        \"valuta_rischio\": \"\",\n",
        "        \"scenari_performance\": {{\n",
        "          \"stress\": {{\n",
        "            \"1_anno\": {{\"rimborso\": \"\", \"rendimento\": \"\"}},\n",
        "            \"5_anni\": {{\"rimborso\": \"\", \"rendimento\": \"\"}},\n",
        "            \"10_anni\": {{\"rimborso\": \"\", \"rendimento\": \"\"}}\n",
        "          }},\n",
        "          \"sfavorevole\": {{\n",
        "            \"1_anno\": {{\"rimborso\": \"\", \"rendimento\": \"\"}},\n",
        "            \"5_anni\": {{\"rimborso\": \"\", \"rendimento\": \"\"}},\n",
        "            \"10_anni\": {{\"rimborso\": \"\", \"rendimento\": \"\"}}\n",
        "          }},\n",
        "          \"moderato\": {{\n",
        "            \"1_anno\": {{\"rimborso\": \"\", \"rendimento\": \"\"}},\n",
        "            \"5_anni\": {{\"rimborso\": \"\", \"rendimento\": \"\"}},\n",
        "            \"10_anni\": {{\"rimborso\": \"\", \"rendimento\": \"\"}}\n",
        "          }},\n",
        "          \"favorevole\": {{\n",
        "            \"1_anno\": {{\"rimborso\": \"\", \"rendimento\": \"\"}},\n",
        "            \"5_anni\": {{\"rimborso\": \"\", \"rendimento\": \"\"}},\n",
        "            \"10_anni\": {{\"rimborso\": \"\", \"rendimento\": \"\"}}\n",
        "          }}\n",
        "        }}\n",
        "      }},\n",
        "      \"cost_structure\": {{\n",
        "        \"costi_ingresso\": {{\n",
        "          \"percentuale_annua\": \"\",\n",
        "          \"descrizione\": \"\"\n",
        "        }},\n",
        "        \"costi_uscita\": {{\n",
        "          \"penali_primi_7_anni\": \"\",\n",
        "          \"percentuale_max\": \"\",\n",
        "          \"descrizione\": \"\"\n",
        "        }},\n",
        "        \"costi_gestione_annui\": {{\n",
        "          \"commissioni_gestione\": \"\",\n",
        "          \"costi_amministrativi\": \"\",\n",
        "          \"costi_transazione\": \"\"\n",
        "        }},\n",
        "        \"incidenza_costi_totali\": {{\n",
        "          \"1_anno\": \"\",\n",
        "          \"5_anni\": \"\",\n",
        "          \"10_anni\": \"\"\n",
        "        }},\n",
        "        \"ric_totale_costi\": {{\n",
        "          \"min\": \"\",\n",
        "          \"max\": \"\"\n",
        "        }}\n",
        "      }},\n",
        "      \"insurance_benefits\": {{\n",
        "        \"copertura_decesso\": {{\n",
        "          \"base\": \"\",\n",
        "          \"formula_calcolo\": \"\",\n",
        "          \"bonus_completamento\": \"\"\n",
        "        }},\n",
        "        \"coperture_aggiuntive\": [\n",
        "          {{\n",
        "            \"nome\": \"\",\n",
        "            \"prestazione\": \"\",\n",
        "            \"premio_annuo\": \"\",\n",
        "            \"descrizione\": \"\"\n",
        "          }}\n",
        "        ],\n",
        "        \"prestazioni_vita\": {{\n",
        "          \"riscatto_disponibile\": \"si/no\",\n",
        "          \"modalita_riscatto\": \"\",\n",
        "          \"vincoli_temporali\": \"\"\n",
        "        }}\n",
        "      }},\n",
        "      \"target_market\": {{\n",
        "        \"profilo_cliente\": {{\n",
        "          \"tipologia\": \"retail/professionale\",\n",
        "          \"esperienza_richiesta\": \"\",\n",
        "          \"orizzonte_temporale\": \"\",\n",
        "          \"profilo_rischio\": \"\",\n",
        "          \"capacita_perdite\": \"\",\n",
        "          \"importo_minimo_premio\": \"\",\n",
        "          \"bisogni_primari\": []\n",
        "        }}\n",
        "      }},\n",
        "      \"actuarial_metrics\": {{\n",
        "        \"ter_medio_10_anni\": \"\",\n",
        "        \"volatilita_rendimenti\": \"\",\n",
        "        \"duration_portafoglio\": \"\",\n",
        "        \"max_drawdown_stimato\": \"\",\n",
        "        \"break_even_anni\": \"\"\n",
        "      }},\n",
        "      \"compliance_flags\": {{\n",
        "        \"costi_elevati\": \"si/no\",\n",
        "        \"complessita_alta\": \"si/no\",\n",
        "        \"rischio_cambio\": \"si/no\",\n",
        "        \"liquidita_limitata\": \"si/no\",\n",
        "        \"mifid_compatibile\": \"si/no\",\n",
        "        \"warnings_presenti\": []\n",
        "      }},\n",
        "      \"descriptive_texts\": {{\n",
        "        \"executive_summary\": \"Brief 2-3 sentence summary of this specific KID's key characteristics\",\n",
        "        \"product_description\": \"Detailed 200-300 word description of this specific product\",\n",
        "        \"risk_assessment\": \"100-150 word analysis of the main risks for this specific product\",\n",
        "        \"cost_analysis\": \"100-150 word explanation of the cost structure for this specific product\",\n",
        "        \"suitability_notes\": \"100-150 word guidance on client suitability for this specific product\"\n",
        "      }},\n",
        "      \"extracted_text_sections\": {{\n",
        "        \"product_objectives\": \"Verbatim text describing this KID's objectives\",\n",
        "        \"investment_policy\": \"Verbatim text describing this KID's investment policy\",\n",
        "        \"risk_warnings\": \"Verbatim risk warnings specific to this KID\",\n",
        "        \"cost_disclosures\": \"Verbatim cost information for this KID\",\n",
        "        \"target_market_description\": \"Verbatim target market text for this KID\"\n",
        "      }},\n",
        "      \"data_quality\": {{\n",
        "        \"completeness_score\": \"1-10\",\n",
        "        \"missing_fields\": [],\n",
        "        \"extraction_confidence\": \"high/medium/low\"\n",
        "      }}\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "1. Identify ALL distinct KID sections in the document automatically\n",
        "2. Do NOT split a single KID into multiple parts\n",
        "3. Extract complete information for each individual KID found\n",
        "4. Generate unique, descriptive kid_id for each KID (use product name + option if available)\n",
        "5. The \"total_kids_found\" field must match the actual number of objects in the \"kids\" array\n",
        "6. Use \"N/A\" for fields not present in the specific KID\n",
        "7. Preserve all numerical precision from the original documents\n",
        "8. Generate engaging, professional descriptive texts specific to each KID\n",
        "9. Include verbatim text extractions from key sections of each KID\n",
        "\n",
        "Return ONLY the JSON response. Be thorough and accurate in identifying distinct KID sections.\"\"\"\n",
        "\n",
        "        # Formato compatibile con il sistema di processing\n",
        "        prompt_json = json.dumps([\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ], ensure_ascii=False)\n",
        "\n",
        "        prompts.append(prompt_json)\n",
        "\n",
        "    # Salva i prompt\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for prompt in prompts:\n",
        "            f.write(prompt + '\\n')\n",
        "\n",
        "    print(f\"Generated {len(prompts)} intelligent analysis prompts for {len(kid_documents)} documents\")\n",
        "    print(f\"Each prompt will automatically detect and extract multiple KIDs per document\")\n",
        "    print(f\"Saved to {output_file}\")\n",
        "\n",
        "    return prompts\n",
        "\n",
        "def load_kid_documents_from_markdown(file_paths):\n",
        "    \"\"\"\n",
        "    Carica documenti KID da file markdown (versione semplificata)\n",
        "\n",
        "    Args:\n",
        "        file_paths (list): Lista di percorsi ai file markdown\n",
        "\n",
        "    Returns:\n",
        "        list: Lista di documenti KID processati\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "\n",
        "    for i, file_path in enumerate(file_paths):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "                documents.append({\n",
        "                    'id': f\"KID_DOC_{i+1}\",\n",
        "                    'file_path': file_path,\n",
        "                    'content': content,\n",
        "                    'content_length': len(content)\n",
        "                })\n",
        "                print(f\"Loaded {file_path}: {len(content)} characters\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "\n",
        "    return documents\n",
        "\n",
        "def process_gpt_responses(response_file, output_file='processed_kid_data.json'):\n",
        "    \"\"\"\n",
        "    Processa le risposte JSON di GPT per aggregare i dati di tutti i KID estratti\n",
        "\n",
        "    Args:\n",
        "        response_file (str): File contenente le risposte JSON di GPT\n",
        "        output_file (str): File di output per i dati aggregati\n",
        "\n",
        "    Returns:\n",
        "        dict: Dati aggregati di tutti i KID processati\n",
        "    \"\"\"\n",
        "    all_kids_data = []\n",
        "    processing_stats = {\n",
        "        'total_documents_processed': 0,\n",
        "        'total_kids_extracted': 0,\n",
        "        'avg_kids_per_document': 0,\n",
        "        'extraction_errors': [],\n",
        "        'data_quality_summary': {}\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with open(response_file, 'r', encoding='utf-8') as f:\n",
        "            for line_num, line in enumerate(f, 1):\n",
        "                try:\n",
        "                    response_data = json.loads(line.strip())\n",
        "\n",
        "                    # Estrai metadati documento\n",
        "                    doc_analysis = response_data.get('document_analysis', {})\n",
        "                    kids_found = doc_analysis.get('total_kids_found', 0)\n",
        "\n",
        "                    processing_stats['total_documents_processed'] += 1\n",
        "                    processing_stats['total_kids_extracted'] += kids_found\n",
        "\n",
        "                    # Aggiungi ogni KID ai dati aggregati\n",
        "                    for kid in response_data.get('kids', []):\n",
        "                        kid['source_line'] = line_num\n",
        "                        kid['source_document'] = doc_analysis.get('source_document_id', f'DOC_{line_num}')\n",
        "                        all_kids_data.append(kid)\n",
        "\n",
        "                except json.JSONDecodeError as e:\n",
        "                    error_msg = f\"JSON decode error at line {line_num}: {e}\"\n",
        "                    processing_stats['extraction_errors'].append(error_msg)\n",
        "                    print(f\"Warning: {error_msg}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    error_msg = f\"Processing error at line {line_num}: {e}\"\n",
        "                    processing_stats['extraction_errors'].append(error_msg)\n",
        "                    print(f\"Error: {error_msg}\")\n",
        "\n",
        "        # Calcola statistiche finali\n",
        "        if processing_stats['total_documents_processed'] > 0:\n",
        "            processing_stats['avg_kids_per_document'] = processing_stats['total_kids_extracted'] / processing_stats['total_documents_processed']\n",
        "\n",
        "        # Analizza qualit√† dati\n",
        "        quality_scores = [int(kid.get('data_quality', {}).get('completeness_score', 0)) for kid in all_kids_data]\n",
        "        if quality_scores:\n",
        "            processing_stats['data_quality_summary'] = {\n",
        "                'avg_completeness': sum(quality_scores) / len(quality_scores),\n",
        "                'min_completeness': min(quality_scores),\n",
        "                'max_completeness': max(quality_scores)\n",
        "            }\n",
        "\n",
        "        # Salva risultati aggregati\n",
        "        final_output = {\n",
        "            'processing_metadata': processing_stats,\n",
        "            'extraction_timestamp': datetime.now().isoformat(),\n",
        "            'total_kids_count': len(all_kids_data),\n",
        "            'kids_data': all_kids_data\n",
        "        }\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(final_output, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"\\nPROCESSING COMPLETE:\")\n",
        "        print(f\"- Documents processed: {processing_stats['total_documents_processed']}\")\n",
        "        print(f\"- Total KIDs extracted: {processing_stats['total_kids_extracted']}\")\n",
        "        print(f\"- Average KIDs per document: {processing_stats['avg_kids_per_document']:.1f}\")\n",
        "        print(f\"- Processing errors: {len(processing_stats['extraction_errors'])}\")\n",
        "        print(f\"- Output saved to: {output_file}\")\n",
        "\n",
        "        if processing_stats['data_quality_summary']:\n",
        "            print(f\"- Average data completeness: {processing_stats['data_quality_summary']['avg_completeness']:.1f}/10\")\n",
        "\n",
        "        return final_output\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Response file {response_file} not found\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing responses: {e}\")\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sJr-3Wt59fPn"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "kid_files = glob.glob('/content/markdown_output/*.md')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Jisso-H9d-O",
        "outputId": "235a1e1b-8f3d-4bad-e838-2422a0400659"
      },
      "outputs": [],
      "source": [
        "# Esempio di utilizzo\n",
        "if __name__ == \"__main__\":\n",
        "    # Carica documenti markdown (possono contenere uno o pi√π KID)\n",
        "    kid_documents = load_kid_documents_from_markdown(kid_files)\n",
        "\n",
        "\n",
        "    # Genera prompt intelligenti (GPT gestisce automaticamente i KID multipli)\n",
        "    prompts = generate_kid_analysis_prompts(kid_documents, 'smart_kid_analysis.txt')\n",
        "\n",
        "    print(f\"\\nSUCCESS: Generated {len(prompts)} intelligent prompts\")\n",
        "    print(\"Each prompt will automatically:\")\n",
        "    print(\"- Detect how many KIDs are in each document\")\n",
        "    print(\"- Extract complete data for each KID found\")\n",
        "    print(\"- Return JSON with proper structure reflecting actual KID count\")\n",
        "\n",
        "    # Esempio con contenuto diretto\n",
        "    sample_content = \"\"\"\n",
        "    Il tuo documento markdown con uno o pi√π KID...\n",
        "    GPT detecter√† automaticamente la struttura!\n",
        "    \"\"\"\n",
        "\n",
        "    direct_prompts = generate_kid_analysis_prompts([sample_content], 'direct_smart_analysis.txt')\n",
        "\n",
        "    # Dopo aver ottenuto le risposte da GPT, processale\n",
        "    # processed_data = process_gpt_responses('gpt_responses.jsonl', 'final_kid_database.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7cLJUvO-xRs"
      },
      "source": [
        "## 5. Estrazione e Pulizia dei Dati\n",
        "\n",
        "Questa sezione del notebook si concentra sulla trasformazione delle informazioni estratte dal modello di linguaggio in un formato strutturato e sulla successiva fase di pulizia e uniformazione dei dati. √à un passaggio cruciale per rendere i dati pronti per l'analisi, la visualizzazione o l'ulteriore elaborazione.\n",
        "\n",
        "### Scopo:\n",
        "\n",
        "L'obiettivo di questa fase √® duplice:\n",
        "1.  **Strutturazione dei dati**: Convertire le risposte non ancora completamente strutturate del modello di linguaggio in un \"DataFrame\" Pandas, che √® una tabella organizzata e facile da manipolare per l'analisi.\n",
        "2.  **Qualit√† e uniformit√† dei dati**: Pulire, standardizzare e arricchire il dataset, risolvendo incoerenze e preparandolo per analisi quantitative.\n",
        "\n",
        "### Dettagli del Processo:\n",
        "\n",
        "Gli output mostrano le seguenti operazioni [1-3]:\n",
        "\n",
        "*   **Creazione del Dataset Completo**: Il processo inizia con la creazione di un dataset completo delle informazioni chiave (KID) estratte (\"üöÄ Creating complete KID dataset...\") [1].\n",
        "*   **Caricamento ed Elaborazione**: I dati chiave vengono caricati e processati. Il notebook indica che sono state elaborate un certo numero di singole informazioni chiave (ad esempio, \"üìä Processed 81 individual KIDs\") [1].\n",
        "*   **Creazione del DataFrame**: Le informazioni elaborate vengono consolidate in un DataFrame Pandas, con indicazione delle sue dimensioni (es. \"üìã Created DataFrame with 81 rows and 171 columns\") [1]. Questo significa che ogni riga rappresenta un'informazione chiave estratta e ogni colonna un attributo di tale informazione.\n",
        "*   **Arricchimento del DataFrame**: Il DataFrame viene ulteriormente migliorato, ad esempio con l'aggiunta di colonne numeriche, utili per analisi quantitative (es. \"üîß Enhancing DataFrame... ‚úÖ DataFrame enhanced with 18 numeric columns\") [1].\n",
        "*   **Uniformazione Avanzata**: Si avvia un processo di uniformazione avanzata del dataset [2]. Questo passaggio √® essenziale per standardizzare i formati dei dati, gestire valori mancanti o inconsistenti e preparare il dataset per l'analisi comparativa. Viene mostrato il confronto tra le dimensioni del \"Dataset originale\" e del \"Dataset pulito\", indicando una riduzione delle colonne ma un aumento delle colonne numeriche a seguito della pulizia e uniformazione (es. \"Dataset originale: 81 righe, 49 colonne\" e \"Dataset pulito: (81, 119) Colonne numeriche: 62\") [2, 3]. Questo indica che alcune colonne non essenziali o ridondanti sono state rimosse o consolidate, mentre nuove colonne numeriche sono state create o identificate.\n",
        "\n",
        "Questa fase √® fondamentale per trasformare i dati grezzi e testuali, estratti dal modello di linguaggio, in un formato strutturato e pulito, essenziale per qualsiasi successiva analisi o modellazione predittiva."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*riestraiamo i risultati ottenuti dalle sezioni precedenti*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fmme5LltAf-K",
        "outputId": "cdd3a9ae-9300-44ca-bb0d-86883286097a"
      },
      "outputs": [],
      "source": [
        "!7z a pdfs.7z downloaded_pdfs/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlcIMstgAx1U",
        "outputId": "3ff643a8-86c2-4a25-b021-3cbc04a11764"
      },
      "outputs": [],
      "source": [
        "!7z a mds.7z markdown_output/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYi2evEgMr5m"
      },
      "source": [
        "# lancio degli prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQULS8ZgA3-0"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import openai\n",
        "import json\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "def clean_and_fix_json_response(response_text):\n",
        "    \"\"\"\n",
        "    Tenta di pulire e riparare risposte JSON malformate da GPT\n",
        "    \"\"\"\n",
        "    if not response_text or not response_text.strip():\n",
        "        return None\n",
        "\n",
        "    # Rimuovi caratteri di controllo e spazi extra\n",
        "    cleaned = response_text.strip()\n",
        "\n",
        "    # Rimuovi eventuali ``` markdown\n",
        "    cleaned = re.sub(r'^```json\\s*', '', cleaned, flags=re.MULTILINE)\n",
        "    cleaned = re.sub(r'^```\\s*$', '', cleaned, flags=re.MULTILINE)\n",
        "\n",
        "    # Rimuovi eventuali commenti o testo extra prima/dopo il JSON\n",
        "    json_start = cleaned.find('{')\n",
        "    json_end = cleaned.rfind('}')\n",
        "\n",
        "    if json_start != -1 and json_end != -1 and json_end > json_start:\n",
        "        cleaned = cleaned[json_start:json_end + 1]\n",
        "\n",
        "    # Tenta di parsare il JSON\n",
        "    try:\n",
        "        parsed = json.loads(cleaned)\n",
        "        return parsed\n",
        "    except json.JSONDecodeError as e:\n",
        "        # Tenta riparazioni comuni\n",
        "        try:\n",
        "            # Riparazione 1: Aggiungi virgole mancanti\n",
        "            fixed = re.sub(r'\"\\s*\\n\\s*\"', '\",\\n\"', cleaned)\n",
        "            parsed = json.loads(fixed)\n",
        "            return parsed\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            # Riparazione 2: Rimuovi virgole trailing\n",
        "            fixed = re.sub(r',\\s*}', '}', cleaned)\n",
        "            fixed = re.sub(r',\\s*]', ']', fixed)\n",
        "            parsed = json.loads(fixed)\n",
        "            return parsed\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Se tutte le riparazioni falliscono, ritorna None\n",
        "        return None\n",
        "\n",
        "def extract_partial_kid_data(response_text):\n",
        "    \"\"\"\n",
        "    Estrae dati parziali anche da risposte JSON malformate\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Cerca pattern specifici nel testo\n",
        "        patterns = {\n",
        "            'nome_prodotto': r'\"nome_prodotto\":\\s*\"([^\"]+)\"',\n",
        "            'compagnia': r'\"compagnia\":\\s*\"([^\"]+)\"',\n",
        "            'indicatore_rischio': r'\"indicatore_rischio_sintetico\":\\s*\"([^\"]+)\"',\n",
        "            'total_kids_found': r'\"total_kids_found\":\\s*(\\d+)'\n",
        "        }\n",
        "\n",
        "        extracted = {}\n",
        "        for key, pattern in patterns.items():\n",
        "            match = re.search(pattern, response_text, re.IGNORECASE)\n",
        "            if match:\n",
        "                extracted[key] = match.group(1)\n",
        "\n",
        "        return extracted if extracted else None\n",
        "\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def estimate_tokens_needed():\n",
        "    \"\"\"\n",
        "    Stima i token necessari per una risposta KID completa\n",
        "    \"\"\"\n",
        "    # Esempio di JSON KID completo tipico\n",
        "    sample_json_structure = {\n",
        "        \"document_analysis\": {\"source_document_id\": \"DOC_1\", \"total_kids_found\": 6},\n",
        "        \"kids\": [\n",
        "            {\n",
        "                \"kid_id\": \"G>>>>Sviluppo_MultiPlan_G>>>>>>_Consumo_>>>>>nsabile_KID_1\",\n",
        "                \"product_identification\": {\n",
        "                    \"nome_prodotto\": \"G>>>>>Sviluppo MultiPlan\",\n",
        "                    \"compagnia\": \"G>>>>>>> ITALIA S.p.A.\",\n",
        "                    \"data_documento\": \"2024-01-15\",\n",
        "                    \"tipo_prodotto\": \"Assicurazione sulla vita di tipo I\",\n",
        "                    \"durata_contratto\": \"Tutta la vita\",\n",
        "                    \"periodo_versamenti\": \"Primo versamento pi√π eventuali versamenti aggiuntivi\"\n",
        "                },\n",
        "                \"descriptive_texts\": {\n",
        "                    \"executive_summary\": \"G>>>>>Sviluppo MultiPlan √® un prodotto assicurativo vita di tipo I che combina protezione assicurativa e opportunit√† di investimento attraverso diverse opzioni di gestione separata e fondi interni, offrendo flessibilit√† nella scelta dell'allocazione degli investimenti con un indicatore di rischio moderato.\",\n",
        "                    \"product_description\": \"Il prodotto G>>>>Sviluppo MultiPlan rappresenta una soluzione assicurativa vita completa che offre al contraente la possibilit√† di combinare protezione del capitale investito con opportunit√† di crescita attraverso diverse opzioni di investimento.\"\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Stima approssimativa: 1 token ‚âà 4 caratteri per l'italiano\n",
        "    sample_text = json.dumps(sample_json_structure, ensure_ascii=False, indent=2)\n",
        "    estimated_tokens_per_kid = len(sample_text) // 4\n",
        "\n",
        "    print(f\"üìè TOKEN ESTIMATION:\")\n",
        "    print(f\"Sample JSON length: {len(sample_text)} characters\")\n",
        "    print(f\"Estimated tokens per KID: {estimated_tokens_per_kid}\")\n",
        "    print(f\"For 6 KIDs: ~{estimated_tokens_per_kid * 6} tokens\")\n",
        "    print(f\"Current max_tokens: 12000\")\n",
        "    print(f\"Recommended max_tokens: {max(10000, estimated_tokens_per_kid * 8)}\")\n",
        "\n",
        "    return estimated_tokens_per_kid\n",
        "\n",
        "async def call_gpt_kid_analysis(prompt_json, client):\n",
        "    \"\"\"Esegue una singola chiamata a GPT per analisi KID con recovery\"\"\"\n",
        "    try:\n",
        "        # Parsa il JSON delle messages\n",
        "        messages = json.loads(prompt_json)\n",
        "\n",
        "        response = await asyncio.to_thread(\n",
        "            client.chat.completions.create,\n",
        "            model=\"gpt-4.1\",\n",
        "            messages=messages,\n",
        "            temperature=0.1,\n",
        "            max_tokens=12000  # Aumentato ulteriormente per gestire JSON molto complessi\n",
        "        )\n",
        "\n",
        "        response_content = response.choices[0].message.content\n",
        "\n",
        "        # Verifica se la risposta √® stata troncata\n",
        "        finish_reason = response.choices[0].finish_reason\n",
        "        if finish_reason == 'length':\n",
        "            print(f\"‚ö†Ô∏è Response truncated due to max_tokens limit\")\n",
        "\n",
        "        # Tenta di pulire e parsare la risposta\n",
        "        cleaned_response = clean_and_fix_json_response(response_content)\n",
        "\n",
        "        if cleaned_response:\n",
        "            kids_found = cleaned_response.get('document_analysis', {}).get('total_kids_found', 0)\n",
        "            validation_status = \"valid_json\"\n",
        "            final_response = json.dumps(cleaned_response, ensure_ascii=False)\n",
        "        else:\n",
        "            # Tenta estrazione parziale\n",
        "            partial_data = extract_partial_kid_data(response_content)\n",
        "            kids_found = 0\n",
        "            validation_status = \"invalid_json\"\n",
        "            final_response = response_content\n",
        "\n",
        "            # Se abbiamo dati parziali, segna come \"partial\"\n",
        "            if partial_data:\n",
        "                validation_status = \"partial_data\"\n",
        "                if 'total_kids_found' in partial_data:\n",
        "                    try:\n",
        "                        kids_found = int(partial_data['total_kids_found'])\n",
        "                    except:\n",
        "                        kids_found = 0\n",
        "\n",
        "        return {\n",
        "            \"prompt\": prompt_json,\n",
        "            \"response\": final_response,\n",
        "            \"raw_response\": response_content,\n",
        "            \"finish_reason\": finish_reason,\n",
        "            \"status\": \"success\",\n",
        "            \"validation\": validation_status,\n",
        "            \"kids_found\": kids_found,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"prompt\": prompt_json,\n",
        "            \"response\": f\"Error: {str(e)}\",\n",
        "            \"raw_response\": \"\",\n",
        "            \"finish_reason\": \"error\",\n",
        "            \"status\": \"error\",\n",
        "            \"validation\": \"error\",\n",
        "            \"kids_found\": 0,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "async def process_kid_prompts(api_key, input_file, output_file, max_concurrent=15):\n",
        "    \"\"\"Processa tutti i prompt KID dal file con recovery avanzato\"\"\"\n",
        "    # Inizializza il client OpenAI\n",
        "    client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "    # Leggi i prompt\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        prompts = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    print(f\"Loaded {len(prompts)} KID analysis prompts\")\n",
        "\n",
        "    # Crea un semaforo per limitare le richieste concorrenti\n",
        "    sem = asyncio.Semaphore(max_concurrent)\n",
        "\n",
        "    async def bounded_call(prompt):\n",
        "        async with sem:\n",
        "            return await call_gpt_kid_analysis(prompt, client)\n",
        "\n",
        "    # Esegui le richieste con progress bar\n",
        "    tasks = [bounded_call(prompt) for prompt in prompts]\n",
        "    results = []\n",
        "\n",
        "    print(\"Processing KID documents with GPT...\")\n",
        "    for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
        "        result = await coro\n",
        "        results.append(result)\n",
        "\n",
        "        # Log progress dettagliato ogni 10 documenti\n",
        "        if len(results) % 10 == 0:\n",
        "            successful = sum(1 for r in results if r['status'] == 'success')\n",
        "            valid_json = sum(1 for r in results if r['validation'] == 'valid_json')\n",
        "            partial_data = sum(1 for r in results if r['validation'] == 'partial_data')\n",
        "            invalid_json = sum(1 for r in results if r['validation'] == 'invalid_json')\n",
        "            truncated = sum(1 for r in results if r.get('finish_reason') == 'length')\n",
        "            total_kids = sum(r['kids_found'] for r in results)\n",
        "\n",
        "            print(f\"Progress: {len(results)}/{len(prompts)}\")\n",
        "            print(f\"  Valid JSON: {valid_json}, Partial: {partial_data}, Invalid: {invalid_json}\")\n",
        "            print(f\"  Truncated responses: {truncated}\")\n",
        "            print(f\"  Total KIDs extracted: {total_kids}\")\n",
        "\n",
        "    # Salva i risultati grezzi\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Salva solo le risposte valide in formato JSONL\n",
        "    jsonl_file = output_file.replace('.json', '_valid.jsonl')\n",
        "    valid_count = 0\n",
        "    with open(jsonl_file, 'w', encoding='utf-8') as f:\n",
        "        for result in results:\n",
        "            if result['validation'] == 'valid_json':\n",
        "                f.write(result['response'] + '\\n')\n",
        "                valid_count += 1\n",
        "\n",
        "    # Salva anche le risposte problematiche per debugging\n",
        "    debug_file = output_file.replace('.json', '_debug.txt')\n",
        "    with open(debug_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"PROBLEMATIC RESPONSES FOR DEBUGGING\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "        for i, result in enumerate(results):\n",
        "            if result['validation'] in ['invalid_json', 'partial_data']:\n",
        "                f.write(f\"RESPONSE {i+1} - Status: {result['validation']}\\n\")\n",
        "                f.write(\"-\" * 30 + \"\\n\")\n",
        "                f.write(result['raw_response'][:500] + \"...\\n\\n\")\n",
        "\n",
        "    # Statistiche finali dettagliate\n",
        "    stats = {\n",
        "        'total_prompts': len(prompts),\n",
        "        'successful_responses': sum(1 for r in results if r['status'] == 'success'),\n",
        "        'valid_json_responses': sum(1 for r in results if r['validation'] == 'valid_json'),\n",
        "        'partial_data_responses': sum(1 for r in results if r['validation'] == 'partial_data'),\n",
        "        'invalid_json_responses': sum(1 for r in results if r['validation'] == 'invalid_json'),\n",
        "        'error_responses': sum(1 for r in results if r['status'] == 'error'),\n",
        "        'truncated_responses': sum(1 for r in results if r.get('finish_reason') == 'length'),\n",
        "        'total_kids_extracted': sum(r['kids_found'] for r in results)\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìä DETAILED PROCESSING SUMMARY:\")\n",
        "    print(f\"Total prompts: {stats['total_prompts']}\")\n",
        "    print(f\"Successful responses: {stats['successful_responses']}\")\n",
        "    print(f\"‚úÖ Valid JSON: {stats['valid_json_responses']}\")\n",
        "    print(f\"üîß Partial data: {stats['partial_data_responses']}\")\n",
        "    print(f\"‚ùå Invalid JSON: {stats['invalid_json_responses']}\")\n",
        "    print(f\"üí• Errors: {stats['error_responses']}\")\n",
        "    print(f\"‚úÇÔ∏è Truncated (need more tokens): {stats['truncated_responses']}\")\n",
        "    print(f\"üìà Total KIDs extracted: {stats['total_kids_extracted']}\")\n",
        "\n",
        "    if stats['truncated_responses'] > 0:\n",
        "        print(f\"\\n‚ö†Ô∏è WARNING: {stats['truncated_responses']} responses were truncated!\")\n",
        "        print(f\"Consider increasing max_tokens beyond 12000 for better results.\")\n",
        "\n",
        "    if stats['valid_json_responses'] > 0:\n",
        "        avg_kids = stats['total_kids_extracted'] / stats['valid_json_responses']\n",
        "        print(f\"üìä Average KIDs per valid document: {avg_kids:.1f}\")\n",
        "\n",
        "    success_rate = stats['successful_responses'] / stats['total_prompts'] * 100\n",
        "    valid_rate = stats['valid_json_responses'] / stats['successful_responses'] * 100 if stats['successful_responses'] > 0 else 0\n",
        "\n",
        "    print(f\"üéØ Success rate: {success_rate:.1f}%\")\n",
        "    print(f\"üéØ JSON validation rate: {valid_rate:.1f}%\")\n",
        "    print(f\"üìÅ Valid responses saved to: {jsonl_file}\")\n",
        "    print(f\"üêõ Debug file created: {debug_file}\")\n",
        "\n",
        "    return results, stats\n",
        "\n",
        "def analyze_kid_extraction_results(results_file):\n",
        "    \"\"\"Analizza i risultati dell'estrazione KID\"\"\"\n",
        "    with open(results_file, 'r', encoding='utf-8') as f:\n",
        "        results = json.load(f)\n",
        "\n",
        "    # Analisi dettagliata\n",
        "    analysis = {\n",
        "        'total_documents': len(results),\n",
        "        'successful_extractions': 0,\n",
        "        'failed_extractions': 0,\n",
        "        'json_parse_errors': 0,\n",
        "        'total_kids_found': 0,\n",
        "        'kids_per_document': [],\n",
        "        'extraction_errors': [],\n",
        "        'data_quality_issues': []\n",
        "    }\n",
        "\n",
        "    for result in results:\n",
        "        if result['status'] == 'success':\n",
        "            analysis['successful_extractions'] += 1\n",
        "\n",
        "            if result['validation'] == 'valid_json':\n",
        "                kids_count = result['kids_found']\n",
        "                analysis['total_kids_found'] += kids_count\n",
        "                analysis['kids_per_document'].append(kids_count)\n",
        "\n",
        "                # Analisi qualit√† dati\n",
        "                try:\n",
        "                    response_data = json.loads(result['response'])\n",
        "                    actual_kids = len(response_data.get('kids', []))\n",
        "                    reported_kids = response_data.get('document_analysis', {}).get('total_kids_found', 0)\n",
        "\n",
        "                    if actual_kids != reported_kids:\n",
        "                        analysis['data_quality_issues'].append({\n",
        "                            'issue': 'kids_count_mismatch',\n",
        "                            'reported': reported_kids,\n",
        "                            'actual': actual_kids\n",
        "                        })\n",
        "                except:\n",
        "                    pass\n",
        "            else:\n",
        "                analysis['json_parse_errors'] += 1\n",
        "        else:\n",
        "            analysis['failed_extractions'] += 1\n",
        "            analysis['extraction_errors'].append(result['response'])\n",
        "\n",
        "    # Statistiche distribuzione\n",
        "    if analysis['kids_per_document']:\n",
        "        analysis['avg_kids_per_doc'] = sum(analysis['kids_per_document']) / len(analysis['kids_per_document'])\n",
        "        analysis['max_kids_per_doc'] = max(analysis['kids_per_document'])\n",
        "        analysis['min_kids_per_doc'] = min(analysis['kids_per_document'])\n",
        "\n",
        "        # Distribuzione\n",
        "        from collections import Counter\n",
        "        distribution = Counter(analysis['kids_per_document'])\n",
        "        analysis['kids_distribution'] = dict(distribution)\n",
        "\n",
        "    print(\"KID EXTRACTION ANALYSIS:\")\n",
        "    print(f\"üìÑ Total documents processed: {analysis['total_documents']}\")\n",
        "    print(f\"‚úÖ Successful extractions: {analysis['successful_extractions']}\")\n",
        "    print(f\"‚ùå Failed extractions: {analysis['failed_extractions']}\")\n",
        "    print(f\"üîß JSON parse errors: {analysis['json_parse_errors']}\")\n",
        "    print(f\"üìä Total KIDs extracted: {analysis['total_kids_found']}\")\n",
        "\n",
        "    if analysis['kids_per_document']:\n",
        "        print(f\"üìà Average KIDs per document: {analysis['avg_kids_per_doc']:.1f}\")\n",
        "        print(f\"üìä KIDs range: {analysis['min_kids_per_doc']} - {analysis['max_kids_per_doc']}\")\n",
        "        print(f\"üìã Distribution: {analysis['kids_distribution']}\")\n",
        "\n",
        "    if analysis['data_quality_issues']:\n",
        "        print(f\"‚ö†Ô∏è Data quality issues: {len(analysis['data_quality_issues'])}\")\n",
        "\n",
        "    return analysis\n",
        "\n",
        "# Per Google Colab - usa questo invece del main normale\n",
        "async def main_kid_processing_colab():\n",
        "    # Configurazione\n",
        "    API_KEY = \"sk-proj-OPENAI-API-KEY\"  # Sostituisci con la tua chiave API\n",
        "\n",
        "    # Percorsi dei file\n",
        "    INPUT_FILE = \"smart_kid_analysis.txt\"  # File generato da generate_kid_analysis_prompts()\n",
        "    OUTPUT_FILE = \"kid_extraction_results.json\"\n",
        "\n",
        "    # Numero massimo di richieste concorrenti (ridotto per analisi complesse)\n",
        "    MAX_CONCURRENT = 15\n",
        "\n",
        "    print(\"üöÄ Starting KID extraction process...\")\n",
        "\n",
        "    # Esegui il processo di estrazione\n",
        "    results, stats = await process_kid_prompts(API_KEY, INPUT_FILE, OUTPUT_FILE, MAX_CONCURRENT)\n",
        "\n",
        "    print(f\"‚úÖ Extraction completed! Results saved in {OUTPUT_FILE}\")\n",
        "\n",
        "    # Analizza i risultati\n",
        "    analysis = analyze_kid_extraction_results(OUTPUT_FILE)\n",
        "\n",
        "    # Mostra un esempio di risposta se disponibile\n",
        "    valid_responses = [r for r in results if r['validation'] == 'valid_json']\n",
        "    if valid_responses:\n",
        "        try:\n",
        "            first_response = json.loads(valid_responses[0][\"response\"])\n",
        "            doc_analysis = first_response.get('document_analysis', {})\n",
        "            kids = first_response.get('kids', [])\n",
        "\n",
        "            print(f\"\\nüìã EXAMPLE EXTRACTION:\")\n",
        "            print(f\"Document: {doc_analysis.get('source_document_id', 'N/A')}\")\n",
        "            print(f\"KIDs found: {doc_analysis.get('total_kids_found', 0)}\")\n",
        "\n",
        "            if kids:\n",
        "                first_kid = kids[0]\n",
        "                product_name = first_kid.get('product_identification', {}).get('nome_prodotto', 'N/A')\n",
        "                risk_level = first_kid.get('risk_return_profile', {}).get('indicatore_rischio_sintetico', 'N/A')\n",
        "                print(f\"First KID: {product_name} (Risk Level: {risk_level})\")\n",
        "\n",
        "                # Mostra summary se disponibile\n",
        "                summary = first_kid.get('descriptive_texts', {}).get('executive_summary', '')\n",
        "                if summary:\n",
        "                    print(f\"Summary: {summary[:150]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Could not parse example response: {e}\")\n",
        "\n",
        "    return results, stats\n",
        "\n",
        "# Esempio per testare i token\n",
        "# estimate_tokens_needed()\n",
        "\n",
        "# Per eseguire in Google Colab:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "eff0100e90be46808eb6f65cf3c00f23",
            "0dd928cafc32440893062f889d50f2cd",
            "560af61faa984785924e76bf84785788",
            "3365b0f19e6c4bb196c79f45025300bc",
            "beecbeddb8ad49fc86c2ba9fc14e9200",
            "53818003388047b19bbb506d0d349426",
            "cedf31be4c504451a5361b1d29eac982",
            "dd1908ba58524c5684cc2d4baa31b97f",
            "c42a58e4a0b144e59cd7326dbd54e7df",
            "fe319c449609441e9aaef2fde689e042",
            "07248eed84c546eba4580d7bb609faef"
          ]
        },
        "id": "u2xD_xnyYNUQ",
        "outputId": "cb74808f-6ac0-43eb-f592-54fa0f9e14d5"
      },
      "outputs": [],
      "source": [
        "results, stats = await main_kid_processing_colab()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH3FKmjleEJ-"
      },
      "source": [
        "## 5. Estrazione e Pulizia dei Dati\n",
        "\n",
        "Questa sezione del notebook si concentra sulla trasformazione delle informazioni estratte dal modello di linguaggio in un formato strutturato e sulla successiva fase di pulizia e uniformazione dei dati. √à un passaggio cruciale per rendere i dati pronti per l'analisi, la visualizzazione o l'ulteriore elaborazione.\n",
        "\n",
        "### Scopo:\n",
        "\n",
        "L'obiettivo di questa fase √® duplice:\n",
        "1.  **Strutturazione dei dati**: Convertire le risposte non ancora completamente strutturate del modello di linguaggio in un \"DataFrame\" Pandas, che √® una tabella organizzata e facile da manipolare per l'analisi.\n",
        "2.  **Qualit√† e uniformit√† dei dati**: Pulire, standardizzare e arricchire il dataset, risolvendo incoerenze e preparandolo per analisi quantitative.\n",
        "\n",
        "### Dettagli del Processo:\n",
        "\n",
        "Gli output mostrano le seguenti operazioni:\n",
        "\n",
        "*   **Creazione del Dataset Completo**: Il processo inizia con la creazione di un dataset completo delle informazioni chiave (KID) estratte (\"üöÄ Creating complete KID dataset...\") [1].\n",
        "*   **Caricamento ed Elaborazione**: I dati chiave vengono caricati e processati. Il notebook indica che sono state elaborate un certo numero di singole informazioni chiave (ad esempio, \"üìä Processed 81 individual KIDs\") [1].\n",
        "*   **Creazione del DataFrame**: Le informazioni elaborate vengono consolidate in un DataFrame Pandas, con indicazione delle sue dimensioni (es. \"üìã Created DataFrame with 81 rows and 171 columns\") [1]. Questo significa che ogni riga rappresenta un'informazione chiave estratta e ogni colonna un attributo di tale informazione.\n",
        "*   **Arricchimento del DataFrame**: Il DataFrame viene ulteriormente migliorato, ad esempio con l'aggiunta di colonne numeriche, utili per analisi quantitative (es. \"üîß Enhancing DataFrame... ‚úÖ DataFrame enhanced with 18 numeric columns\") [1].\n",
        "*   **Uniformazione Avanzata**: Si avvia un processo di uniformazione avanzata del dataset (\"Inizio uniformazione avanzata dataset KID...\") [2]. Questo passaggio √® essenziale per standardizzare i formati dei dati, gestire valori mancanti o inconsistenti e preparare il dataset per l'analisi comparativa. Viene mostrato il confronto tra le dimensioni del \"Dataset originale\" (ad esempio, \"81 righe, 49 colonne\") e del \"Dataset pulito\" (ad esempio, \"(81, 119) Colonne numeriche: 62\") [2]. Questo indica che alcune colonne non essenziali o ridondanti sono state rimosse o consolidate, mentre nuove colonne numeriche sono state create o identificate.\n",
        "\n",
        "Questa fase √® fondamentale per trasformare i dati grezzi e testuali, estratti dal modello di linguaggio, in un formato strutturato e pulito, essenziale per qualsiasi successiva analisi o modellazione predittiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmwVvce3YPQs",
        "outputId": "1712743b-d0bb-4027-e381-2eba30ad98ed"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "def flatten_nested_dict(nested_dict, parent_key='', sep='_'):\n",
        "    \"\"\"\n",
        "    Appiattisce un dizionario nested in un dizionario flat\n",
        "    \"\"\"\n",
        "    items = []\n",
        "    for key, value in nested_dict.items():\n",
        "        new_key = f\"{parent_key}{sep}{key}\" if parent_key else key\n",
        "\n",
        "        if isinstance(value, dict):\n",
        "            items.extend(flatten_nested_dict(value, new_key, sep=sep).items())\n",
        "        elif isinstance(value, list):\n",
        "            # Per le liste, crea colonne separate per ogni elemento\n",
        "            if value and isinstance(value[0], dict):\n",
        "                # Lista di dizionari (es. opzioni_investimento)\n",
        "                for i, item in enumerate(value):\n",
        "                    if isinstance(item, dict):\n",
        "                        items.extend(flatten_nested_dict(item, f\"{new_key}_{i}\", sep=sep).items())\n",
        "                    else:\n",
        "                        items.append((f\"{new_key}_{i}\", item))\n",
        "            else:\n",
        "                # Lista semplice, converti in stringa\n",
        "                items.append((new_key, ' | '.join(map(str, value)) if value else ''))\n",
        "        else:\n",
        "            items.append((new_key, value))\n",
        "\n",
        "    return dict(items)\n",
        "\n",
        "def extract_numeric_value(value_str):\n",
        "    \"\"\"\n",
        "    Estrae valori numerici da stringhe (es. \"5.2%\" -> 5.2)\n",
        "    \"\"\"\n",
        "    if pd.isna(value_str) or value_str == '' or value_str == 'N/A':\n",
        "        return np.nan\n",
        "\n",
        "    # Rimuovi caratteri non numerici eccetto . , - %\n",
        "    cleaned = re.sub(r'[^\\d.,%\\-]', '', str(value_str))\n",
        "\n",
        "    # Estrai il primo numero trovato\n",
        "    number_match = re.search(r'[\\-]?[\\d,]+\\.?\\d*', cleaned)\n",
        "    if number_match:\n",
        "        number_str = number_match.group().replace(',', '')\n",
        "        try:\n",
        "            return float(number_str)\n",
        "        except ValueError:\n",
        "            return np.nan\n",
        "\n",
        "    return np.nan\n",
        "\n",
        "def convert_kids_to_dataframe(jsonl_file):\n",
        "    \"\"\"\n",
        "    Converte il file JSONL di KID estratti in un DataFrame pandas strutturato\n",
        "\n",
        "    Args:\n",
        "        jsonl_file (str): Path al file JSONL con i KID validi\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame completo con tutti i KID\n",
        "    \"\"\"\n",
        "\n",
        "    all_kids = []\n",
        "    processing_errors = []\n",
        "\n",
        "    print(\"üîÑ Loading and processing KID data...\")\n",
        "\n",
        "    # Leggi tutti i KID dal file JSONL\n",
        "    with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            try:\n",
        "                response_data = json.loads(line.strip())\n",
        "\n",
        "                # Estrai metadati documento\n",
        "                doc_analysis = response_data.get('document_analysis', {})\n",
        "                source_doc = doc_analysis.get('source_document_id', f'DOC_{line_num}')\n",
        "\n",
        "                # Processa ogni KID nel documento\n",
        "                for kid_data in response_data.get('kids', []):\n",
        "                    try:\n",
        "                        # Aggiungi metadati documento\n",
        "                        kid_data['source_document_id'] = source_doc\n",
        "                        kid_data['extraction_line'] = line_num\n",
        "                        kid_data['processing_timestamp'] = datetime.now().isoformat()\n",
        "\n",
        "                        # Appiattisci la struttura nested\n",
        "                        flattened_kid = flatten_nested_dict(kid_data)\n",
        "\n",
        "                        all_kids.append(flattened_kid)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        processing_errors.append({\n",
        "                            'line': line_num,\n",
        "                            'error': str(e),\n",
        "                            'kid_id': kid_data.get('kid_id', 'unknown')\n",
        "                        })\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                processing_errors.append({\n",
        "                    'line': line_num,\n",
        "                    'error': f'JSON decode error: {e}',\n",
        "                    'kid_id': 'parse_error'\n",
        "                })\n",
        "\n",
        "    print(f\"üìä Processed {len(all_kids)} individual KIDs\")\n",
        "    if processing_errors:\n",
        "        print(f\"‚ö†Ô∏è {len(processing_errors)} processing errors encountered\")\n",
        "\n",
        "    # Crea DataFrame\n",
        "    df = pd.DataFrame(all_kids)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"‚ùå No valid KID data found!\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(f\"üìã Created DataFrame with {len(df)} rows and {len(df.columns)} columns\")\n",
        "\n",
        "    # Post-processing per migliorare i dati\n",
        "    df = enhance_dataframe(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "def enhance_dataframe(df):\n",
        "    \"\"\"\n",
        "    Migliora il DataFrame con conversioni di tipo e colonne calcolate\n",
        "    \"\"\"\n",
        "    print(\"üîß Enhancing DataFrame...\")\n",
        "\n",
        "    # Conversioni numeriche per colonne finanziarie chiave\n",
        "    numeric_columns = [\n",
        "        'risk_return_profile_indicatore_rischio_sintetico',\n",
        "        'cost_structure_costi_ingresso_percentuale_annua',\n",
        "        'cost_structure_incidenza_costi_totali_1_anno',\n",
        "        'cost_structure_incidenza_costi_totali_5_anni',\n",
        "        'cost_structure_incidenza_costi_totali_10_anni',\n",
        "        'data_quality_completeness_score'\n",
        "    ]\n",
        "\n",
        "    for col in numeric_columns:\n",
        "        if col in df.columns:\n",
        "            df[f'{col}_numeric'] = df[col].apply(extract_numeric_value)\n",
        "\n",
        "    # Estrazioni specifiche per scenari performance\n",
        "    scenario_columns = [col for col in df.columns if 'scenari_performance' in col and 'rendimento' in col]\n",
        "    for col in scenario_columns:\n",
        "        if col in df.columns:\n",
        "            df[f'{col}_numeric'] = df[col].apply(extract_numeric_value)\n",
        "\n",
        "    # Conversioni booleane\n",
        "    boolean_columns = [\n",
        "        'risk_return_profile_protezione_capitale',\n",
        "        'insurance_benefits_prestazioni_vita_riscatto_disponibile'\n",
        "    ]\n",
        "\n",
        "    for col in boolean_columns:\n",
        "        if col in df.columns:\n",
        "            df[f'{col}_bool'] = df[col].map({\n",
        "                'si': True, 's√¨': True, 'yes': True, 'true': True, 'True': True,\n",
        "                'no': False, 'false': False, 'False': False\n",
        "            })\n",
        "\n",
        "    # Colonne calcolate\n",
        "    if 'data_quality_completeness_score_numeric' in df.columns:\n",
        "        df['data_quality_high'] = df['data_quality_completeness_score_numeric'] >= 8\n",
        "\n",
        "    # Categorizzazione livello rischio\n",
        "    if 'risk_return_profile_indicatore_rischio_sintetico_numeric' in df.columns:\n",
        "        df['risk_category'] = pd.cut(\n",
        "            df['risk_return_profile_indicatore_rischio_sintetico_numeric'],\n",
        "            bins=[0, 2, 4, 6, 7],\n",
        "            labels=['Basso', 'Medio-Basso', 'Medio-Alto', 'Alto'],\n",
        "            include_lowest=True\n",
        "        )\n",
        "\n",
        "    # Pulizia nomi colonne\n",
        "    df.columns = df.columns.str.replace('_', ' ').str.title()\n",
        "\n",
        "    print(f\"‚úÖ DataFrame enhanced with {len([c for c in df.columns if 'numeric' in c.lower()])} numeric columns\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_summary_statistics(df):\n",
        "    \"\"\"\n",
        "    Crea statistiche riassuntive del dataset KID\n",
        "    \"\"\"\n",
        "    print(\"\\nüìà DATASET SUMMARY STATISTICS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Statistiche generali\n",
        "    print(f\"üìä Total KIDs: {len(df)}\")\n",
        "    print(f\"üìÑ Unique source documents: {df['Source Document Id'].nunique()}\")\n",
        "    print(f\"üè¢ Unique companies: {df['Product Identification Compagnia'].nunique()}\")\n",
        "\n",
        "    # Distribuzione prodotti\n",
        "    if 'Product Identification Nome Prodotto' in df.columns:\n",
        "        product_counts = df['Product Identification Nome Prodotto'].value_counts().head(10)\n",
        "        print(f\"\\nüèÜ TOP 10 PRODUCTS:\")\n",
        "        for product, count in product_counts.items():\n",
        "            print(f\"  {product}: {count}\")\n",
        "\n",
        "    # Distribuzione rischio\n",
        "    if 'Risk Category' in df.columns:\n",
        "        risk_dist = df['Risk Category'].value_counts()\n",
        "        print(f\"\\n‚ö†Ô∏è RISK DISTRIBUTION:\")\n",
        "        for risk, count in risk_dist.items():\n",
        "            print(f\"  {risk}: {count} ({count/len(df)*100:.1f}%)\")\n",
        "\n",
        "    # Statistiche qualit√† dati\n",
        "    if 'Data Quality Completeness Score Numeric' in df.columns:\n",
        "        quality_stats = df['Data Quality Completeness Score Numeric'].describe()\n",
        "        print(f\"\\nüìã DATA QUALITY STATS:\")\n",
        "        print(f\"  Average completeness: {quality_stats['mean']:.1f}/10\")\n",
        "        print(f\"  Min-Max: {quality_stats['min']:.0f}-{quality_stats['max']:.0f}\")\n",
        "\n",
        "        high_quality = (df['Data Quality Completeness Score Numeric'] >= 8).sum()\n",
        "        print(f\"  High quality (8+): {high_quality} ({high_quality/len(df)*100:.1f}%)\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def save_enhanced_dataset(df, output_file='kid_complete_dataset.xlsx'):\n",
        "    \"\"\"\n",
        "    Salva il dataset in formato Excel con fogli separati\n",
        "    \"\"\"\n",
        "    print(f\"\\nüíæ Saving enhanced dataset to {output_file}...\")\n",
        "\n",
        "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
        "        # Foglio principale con tutti i dati\n",
        "        df.to_excel(writer, sheet_name='All KIDs', index=False)\n",
        "\n",
        "        # Foglio riassuntivo con statistiche chiave\n",
        "        summary_cols = [\n",
        "            'Kid Id', 'Source Document Id',\n",
        "            'Product Identification Nome Prodotto',\n",
        "            'Product Identification Compagnia',\n",
        "            'Risk Return Profile Indicatore Rischio Sintetico Numeric',\n",
        "            'Risk Category',\n",
        "            'Cost Structure Incidenza Costi Totali 1 Anno Numeric',\n",
        "            'Data Quality Completeness Score Numeric',\n",
        "            'Descriptive Texts Executive Summary'\n",
        "        ]\n",
        "\n",
        "        summary_df = df[[col for col in summary_cols if col in df.columns]].copy()\n",
        "        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
        "\n",
        "        # Foglio con solo dati numerici per analisi\n",
        "        numeric_cols = [col for col in df.columns if 'numeric' in col.lower() or 'bool' in col.lower()]\n",
        "        if numeric_cols:\n",
        "            numeric_df = df[['Kid Id'] + numeric_cols].copy()\n",
        "            numeric_df.to_excel(writer, sheet_name='Numeric Data', index=False)\n",
        "\n",
        "    print(f\"‚úÖ Dataset saved with {len(df)} KIDs across multiple sheets\")\n",
        "\n",
        "def create_concise_kid_dataset(df_full):\n",
        "    \"\"\"\n",
        "    Crea un DataFrame conciso con solo le informazioni pi√π importanti\n",
        "\n",
        "    Args:\n",
        "        df_full (pd.DataFrame): DataFrame completo con tutte le colonne\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame conciso con ~30 colonne chiave\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"‚úÇÔ∏è Creating concise KID dataset with key information only...\")\n",
        "\n",
        "    # Definisci le colonne pi√π importanti per analisi business\n",
        "    key_columns = {\n",
        "        # Identificativi\n",
        "        'kid_id': 'KID_ID',\n",
        "        'source_document_id': 'Source_Document',\n",
        "\n",
        "        # Prodotto base\n",
        "        'product_identification_nome_prodotto': 'Product_Name',\n",
        "        'product_identification_compagnia': 'Company',\n",
        "        'product_identification_tipo_prodotto': 'Product_Type',\n",
        "        'product_identification_durata_contratto': 'Contract_Duration',\n",
        "\n",
        "        # Struttura investimento (prima opzione)\n",
        "        'investment_structure_opzioni_investimento_0_nome_opzione': 'Investment_Option_Name',\n",
        "        'investment_structure_opzioni_investimento_0_tipologia': 'Investment_Type',\n",
        "        'investment_structure_opzioni_investimento_0_livello_rischio': 'Investment_Risk_Level',\n",
        "\n",
        "        # Profilo rischio-rendimento\n",
        "        'risk_return_profile_indicatore_rischio_sintetico': 'Risk_Indicator',\n",
        "        'risk_return_profile_indicatore_rischio_sintetico_numeric': 'Risk_Level_Numeric',\n",
        "        'risk_category': 'Risk_Category',\n",
        "        'risk_return_profile_periodo_detenzione_raccomandato': 'Recommended_Holding_Period',\n",
        "        'risk_return_profile_protezione_capitale': 'Capital_Protection',\n",
        "\n",
        "        # Scenari performance chiave (moderato e favorevole)\n",
        "        'risk_return_profile_scenari_performance_moderato_1_anno_rendimento': 'Return_1Y_Moderate',\n",
        "        'risk_return_profile_scenari_performance_moderato_5_anni_rendimento': 'Return_5Y_Moderate',\n",
        "        'risk_return_profile_scenari_performance_favorevole_1_anno_rendimento': 'Return_1Y_Favorable',\n",
        "        'risk_return_profile_scenari_performance_favorevole_5_anni_rendimento': 'Return_5Y_Favorable',\n",
        "\n",
        "        # Costi chiave\n",
        "        'cost_structure_costi_ingresso_percentuale_annua': 'Entry_Costs_Pct',\n",
        "        'cost_structure_costi_gestione_annui_commissioni_gestione': 'Management_Fees',\n",
        "        'cost_structure_incidenza_costi_totali_1_anno': 'Total_Costs_1Y',\n",
        "        'cost_structure_incidenza_costi_totali_5_anni': 'Total_Costs_5Y',\n",
        "        'cost_structure_incidenza_costi_totali_10_anni': 'Total_Costs_10Y',\n",
        "\n",
        "        # Benefici assicurativi\n",
        "        'insurance_benefits_copertura_decesso_base': 'Death_Benefit_Base',\n",
        "        'insurance_benefits_prestazioni_vita_riscatto_disponibile': 'Surrender_Available',\n",
        "\n",
        "        # Target market\n",
        "        'target_market_profilo_cliente_tipologia': 'Client_Type',\n",
        "        'target_market_profilo_cliente_orizzonte_temporale': 'Time_Horizon',\n",
        "        'target_market_profilo_cliente_importo_minimo_premio': 'Min_Premium',\n",
        "\n",
        "        # Testi descrittivi chiave - COMPLETI\n",
        "        'descriptive_texts_executive_summary': 'Executive_Summary',\n",
        "        'descriptive_texts_risk_assessment': 'Risk_Assessment',\n",
        "        'descriptive_texts_product_description': 'Product_Description',\n",
        "\n",
        "        # Qualit√† dati\n",
        "        'data_quality_completeness_score': 'Data_Quality_Score',\n",
        "        'data_quality_completeness_score_numeric': 'Data_Quality_Numeric',\n",
        "        'data_quality_extraction_confidence': 'Extraction_Confidence'\n",
        "    }\n",
        "\n",
        "    # Seleziona solo le colonne che esistono nel DataFrame\n",
        "    available_columns = {}\n",
        "    missing_columns = []\n",
        "\n",
        "    for old_name, new_name in key_columns.items():\n",
        "        # Cerca la colonna con match flessibile (case insensitive, spazi/underscore)\n",
        "        matched_col = find_column_match(df_full, old_name)\n",
        "        if matched_col:\n",
        "            available_columns[matched_col] = new_name\n",
        "        else:\n",
        "            missing_columns.append(old_name)\n",
        "\n",
        "    print(f\"üìã Found {len(available_columns)} out of {len(key_columns)} key columns\")\n",
        "    if missing_columns:\n",
        "        print(f\"‚ö†Ô∏è Missing columns: {len(missing_columns)}\")\n",
        "\n",
        "    # Crea DataFrame conciso\n",
        "    df_concise = df_full[list(available_columns.keys())].copy()\n",
        "    df_concise = df_concise.rename(columns=available_columns)\n",
        "\n",
        "    # Post-processing per migliorare i dati concisi\n",
        "    df_concise = enhance_concise_dataframe(df_concise)\n",
        "\n",
        "    print(f\"‚úÖ Created concise dataset: {len(df_concise)} rows √ó {len(df_concise.columns)} columns\")\n",
        "\n",
        "    return df_concise\n",
        "\n",
        "def find_column_match(df, target_column):\n",
        "    \"\"\"\n",
        "    Trova una colonna nel DataFrame con match flessibile\n",
        "    \"\"\"\n",
        "    # Normalizza nome target\n",
        "    target_normalized = target_column.lower().replace('_', ' ').replace('-', ' ')\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_normalized = col.lower().replace('_', ' ').replace('-', ' ')\n",
        "        if target_normalized in col_normalized or col_normalized in target_normalized:\n",
        "            return col\n",
        "    return None\n",
        "\n",
        "def enhance_concise_dataframe(df):\n",
        "    \"\"\"\n",
        "    Migliora il DataFrame conciso con conversioni e colonne calcolate\n",
        "    \"\"\"\n",
        "    print(\"üîß Enhancing concise DataFrame...\")\n",
        "\n",
        "    # Conversioni numeriche per costi e rendimenti\n",
        "    numeric_cols = [\n",
        "        'Risk_Level_Numeric', 'Entry_Costs_Pct', 'Total_Costs_1Y', 'Total_Costs_5Y',\n",
        "        'Total_Costs_10Y', 'Data_Quality_Numeric'\n",
        "    ]\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            df[f'{col}_Clean'] = df[col].apply(extract_numeric_value)\n",
        "\n",
        "    # Conversioni per rendimenti\n",
        "    return_cols = [col for col in df.columns if 'Return_' in col]\n",
        "    for col in return_cols:\n",
        "        if col in df.columns:\n",
        "            df[f'{col}_Numeric'] = df[col].apply(extract_numeric_value)\n",
        "\n",
        "    # Categorizzazioni semplificate\n",
        "    if 'Risk_Level_Numeric_Clean' in df.columns:\n",
        "        df['Risk_Simple'] = df['Risk_Level_Numeric_Clean'].map({\n",
        "            1: 'Very Low', 2: 'Low', 3: 'Medium-Low',\n",
        "            4: 'Medium', 5: 'Medium-High', 6: 'High', 7: 'Very High'\n",
        "        })\n",
        "\n",
        "    # Flag per caratteristiche importanti\n",
        "    if 'Capital_Protection' in df.columns:\n",
        "        df['Has_Capital_Protection'] = df['Capital_Protection'].str.lower().isin(['si', 's√¨', 'yes', 'true'])\n",
        "\n",
        "    if 'Total_Costs_1Y_Clean' in df.columns:\n",
        "        df['High_Costs'] = df['Total_Costs_1Y_Clean'] > 3.0  # Costi > 3%\n",
        "\n",
        "    if 'Data_Quality_Numeric_Clean' in df.columns:\n",
        "        df['Reliable_Data'] = df['Data_Quality_Numeric_Clean'] >= 8\n",
        "\n",
        "    # Pulizia testi - MANTIENI TESTI COMPLETI\n",
        "    # Rimuoviamo il troncamento automatico per preservare informazioni importanti\n",
        "    if 'Executive_Summary' in df.columns:\n",
        "        # Mantieni il testo completo, rimuovi solo caratteri problematici\n",
        "        df['Executive_Summary_Clean'] = df['Executive_Summary'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.strip()\n",
        "\n",
        "    if 'Risk_Assessment' in df.columns:\n",
        "        df['Risk_Assessment_Clean'] = df['Risk_Assessment'].str.replace('\\n', ' ').str.replace('\\r', ' ').str.strip()\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_business_insights(df_concise):\n",
        "    \"\"\"\n",
        "    Crea insights business dal DataFrame conciso\n",
        "    \"\"\"\n",
        "    print(\"\\nüìä BUSINESS INSIGHTS FROM CONCISE DATASET\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    insights = {}\n",
        "\n",
        "    # 1. Distribuzione per livello di rischio\n",
        "    if 'Risk_Simple' in df_concise.columns:\n",
        "        risk_dist = df_concise['Risk_Simple'].value_counts()\n",
        "        insights['risk_distribution'] = risk_dist\n",
        "        print(\"üéØ RISK DISTRIBUTION:\")\n",
        "        for risk, count in risk_dist.items():\n",
        "            print(f\"  {risk}: {count} products ({count/len(df_concise)*100:.1f}%)\")\n",
        "\n",
        "    # 2. Prodotti pi√π comuni\n",
        "    if 'Product_Name' in df_concise.columns:\n",
        "        product_dist = df_concise['Product_Name'].value_counts().head(5)\n",
        "        insights['top_products'] = product_dist\n",
        "        print(f\"\\nüèÜ TOP 5 PRODUCTS:\")\n",
        "        for product, count in product_dist.items():\n",
        "            print(f\"  {product}: {count} variants\")\n",
        "\n",
        "    # 3. Analisi costi\n",
        "    if 'Total_Costs_1Y_Clean' in df_concise.columns:\n",
        "        costs_stats = df_concise['Total_Costs_1Y_Clean'].describe()\n",
        "        insights['cost_statistics'] = costs_stats\n",
        "        print(f\"\\nüí∞ COST ANALYSIS (1-Year):\")\n",
        "        print(f\"  Average: {costs_stats['mean']:.2f}%\")\n",
        "        print(f\"  Range: {costs_stats['min']:.2f}% - {costs_stats['max']:.2f}%\")\n",
        "        print(f\"  Median: {costs_stats['50%']:.2f}%\")\n",
        "\n",
        "        high_cost_count = (df_concise['Total_Costs_1Y_Clean'] > 3.0).sum()\n",
        "        print(f\"  High cost products (>3%): {high_cost_count} ({high_cost_count/len(df_concise)*100:.1f}%)\")\n",
        "\n",
        "    # 4. Protezione capitale\n",
        "    if 'Has_Capital_Protection' in df_concise.columns:\n",
        "        protection_count = df_concise['Has_Capital_Protection'].sum()\n",
        "        insights['capital_protection'] = protection_count\n",
        "        print(f\"\\nüõ°Ô∏è CAPITAL PROTECTION:\")\n",
        "        print(f\"  Protected products: {protection_count} ({protection_count/len(df_concise)*100:.1f}%)\")\n",
        "\n",
        "    # 5. Qualit√† dati\n",
        "    if 'Reliable_Data' in df_concise.columns:\n",
        "        reliable_count = df_concise['Reliable_Data'].sum()\n",
        "        insights['data_quality'] = reliable_count\n",
        "        print(f\"\\nüìä DATA QUALITY:\")\n",
        "        print(f\"  High quality extractions: {reliable_count} ({reliable_count/len(df_concise)*100:.1f}%)\")\n",
        "\n",
        "    return insights\n",
        "\n",
        "def save_concise_dataset(df_concise, insights, output_file='kid_concise_dataset.xlsx'):\n",
        "    \"\"\"\n",
        "    Salva il dataset conciso con insights\n",
        "    \"\"\"\n",
        "    print(f\"\\nüíæ Saving concise dataset to {output_file}...\")\n",
        "\n",
        "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
        "        # Foglio principale conciso\n",
        "        df_concise.to_excel(writer, sheet_name='Sheet1', index=False)\n",
        "\n",
        "        # Foglio con solo dati numerici per analisi\n",
        "        numeric_cols = [col for col in df_concise.columns if any(x in col.lower() for x in ['numeric', 'clean', 'level'])]\n",
        "        key_cols = ['KID_ID', 'Product_Name', 'Company', 'Risk_Simple']\n",
        "        analysis_cols = key_cols + [col for col in numeric_cols if col in df_concise.columns]\n",
        "\n",
        "        if analysis_cols:\n",
        "            df_analysis = df_concise[analysis_cols].copy()\n",
        "            df_analysis.to_excel(writer, sheet_name='Analysis Ready', index=False)\n",
        "\n",
        "        # Foglio insights\n",
        "        insights_data = []\n",
        "        for key, value in insights.items():\n",
        "            if hasattr(value, 'items'):  # Series o dict\n",
        "                for item, count in value.items():\n",
        "                    insights_data.append({'Category': key, 'Item': item, 'Count': count})\n",
        "            else:\n",
        "                insights_data.append({'Category': key, 'Item': 'Total', 'Count': value})\n",
        "\n",
        "        if insights_data:\n",
        "            df_insights = pd.DataFrame(insights_data)\n",
        "            df_insights.to_excel(writer, sheet_name='Business Insights', index=False)\n",
        "\n",
        "    print(f\"‚úÖ Concise dataset saved with {len(df_concise)} KIDs and business insights\")\n",
        "\n",
        "# Funzione principale per dataset conciso\n",
        "def create_concise_kid_analysis(df_full, output_file='kid_concise_dataset.xlsx'):\n",
        "    \"\"\"\n",
        "    Crea un dataset conciso e analisi business dai KID estratti\n",
        "\n",
        "    Args:\n",
        "        df_full (pd.DataFrame): DataFrame completo\n",
        "        output_file (str): File di output\n",
        "\n",
        "    Returns:\n",
        "        tuple: (DataFrame conciso, insights business)\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üéØ Creating concise KID dataset for business analysis...\")\n",
        "\n",
        "    # Crea dataset conciso\n",
        "    df_concise = create_concise_kid_dataset(df_full)\n",
        "\n",
        "    # Genera insights business\n",
        "    insights = create_business_insights(df_concise)\n",
        "\n",
        "    # Salva tutto\n",
        "    save_concise_dataset(df_concise, insights, output_file)\n",
        "\n",
        "    print(f\"\\nüéâ CONCISE DATASET CREATED!\")\n",
        "    print(f\"üìä {len(df_concise)} KIDs √ó {len(df_concise.columns)} key columns\")\n",
        "    print(f\"üìÅ Saved to: {output_file}\")\n",
        "    print(f\"üìà Business insights included\")\n",
        "\n",
        "    return df_concise, insights\n",
        "\n",
        "# Funzione principale\n",
        "def create_complete_kid_dataset(jsonl_file='kid_extraction_results_valid.jsonl',\n",
        "                               output_file='kid_complete_dataset.xlsx'):\n",
        "    \"\"\"\n",
        "    Funzione principale per convertire tutti i KID in un dataset pandas completo\n",
        "\n",
        "    Args:\n",
        "        jsonl_file (str): File JSONL con i KID estratti\n",
        "        output_file (str): File Excel di output\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Dataset completo\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üöÄ Creating complete KID dataset...\")\n",
        "\n",
        "    # Converti JSON in DataFrame\n",
        "    df = convert_kids_to_dataframe(jsonl_file)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"‚ùå No data to process!\")\n",
        "        return df\n",
        "\n",
        "    # Crea statistiche riassuntive\n",
        "    df = create_summary_statistics(df)\n",
        "\n",
        "    # Salva dataset completo\n",
        "    save_enhanced_dataset(df, output_file)\n",
        "\n",
        "    print(f\"\\nüéâ SUCCESS! Complete KID dataset created:\")\n",
        "    print(f\"üìä {len(df)} KIDs processed\")\n",
        "    print(f\"üìÅ Saved to: {output_file}\")\n",
        "    print(f\"üìã Columns: {len(df.columns)}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Esempio di utilizzo completo\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Crea il dataset completo\n",
        "    df_full = create_complete_kid_dataset(\n",
        "        jsonl_file='kid_extraction_results_valid.jsonl',\n",
        "        output_file='kid_complete_dataset.xlsx'\n",
        "    )\n",
        "\n",
        "    # 2. Crea il dataset conciso\n",
        "    df_concise, insights = create_concise_kid_analysis(\n",
        "        df_full,\n",
        "        output_file='kid_concise_dataset.xlsx'\n",
        "    )\n",
        "\n",
        "    print(f\"\\nüìã FINAL SUMMARY:\")\n",
        "    print(f\"Full dataset: {df_full.shape[0]} rows √ó {df_full.shape[1]} columns\")\n",
        "    print(f\"Concise dataset: {df_concise.shape[0]} rows √ó {df_concise.shape[1]} columns\")\n",
        "    print(f\"Data reduction: {(1 - df_concise.shape[1]/df_full.shape[1])*100:.1f}% fewer columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "W5REQJPfeLwV"
      },
      "outputs": [],
      "source": [
        "# salvo in excel\n",
        "df_full.to_excel('kid_complete_dataset.xlsx', index=False)\n",
        "df_concise.to_excel('kid_concise_dataset.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uw03eJtolSG",
        "outputId": "7e393d25-67b7-4a2d-9b52-057cd10f967c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inizio uniformazione avanzata dataset KID...\n",
            "=== UNIFORMAZIONE DATASET KID ASSICURATIVO (VERSIONE AVANZATA) ===\n",
            "Dataset originale: 81 righe, 49 colonne\n",
            "\n",
            "1. Uniformazione Product_Type...\n",
            "2. Uniformazione Investment_Type...\n",
            "3. Uniformazione Risk_Category...\n",
            "4. Uniformazione Capital_Protection...\n",
            "5. Uniformazione Client_Type...\n",
            "6. Uniformazione Contract_Duration...\n",
            "7. Uniformazione Time_Horizon...\n",
            "8. Uniformazione Surrender_Available...\n",
            "9. Pulizia avanzata variabili numeriche con gestione range...\n",
            "   Processando Risk_Level_Numeric...\n",
            "   Processando Entry_Costs_Pct...\n",
            "   Processando Management_Fees...\n",
            "   Processando Total_Costs_1Y...\n",
            "   Processando Total_Costs_5Y...\n",
            "   Processando Total_Costs_10Y...\n",
            "   Processando Return_1Y_Moderate...\n",
            "   Processando Return_5Y_Moderate...\n",
            "   Processando Return_1Y_Favorable...\n",
            "   Processando Return_5Y_Favorable...\n",
            "   Processando Min_Premium...\n",
            "10. Creazione variabili derivate...\n",
            "11. Gestione Management_Fees condizionali...\n",
            "\n",
            "Dataset finale: 81 righe, 119 colonne\n",
            "Nuove colonne create: ['Risk_Level_Numeric_Clean', 'Entry_Costs_Pct_Clean', 'Total_Costs_1Y_Clean', 'Total_Costs_5Y_Clean', 'Total_Costs_10Y_Clean', 'Data_Quality_Numeric_Clean', 'Executive_Summary_Clean', 'Risk_Assessment_Clean', 'Product_Type_Clean', 'Investment_Type_Clean', 'Risk_Category_Clean', 'Capital_Protection_Clean', 'Client_Type_Clean', 'Contract_Duration_Clean', 'Time_Horizon_Clean', 'Surrender_Available_Clean', 'Risk_Level_Numeric_Min', 'Risk_Level_Numeric_Max', 'Entry_Costs_Pct_Min', 'Entry_Costs_Pct_Max', 'Management_Fees_Min', 'Management_Fees_Max', 'Management_Fees_Clean', 'Total_Costs_1Y_Min', 'Total_Costs_1Y_Max', 'Total_Costs_5Y_Min', 'Total_Costs_5Y_Max', 'Total_Costs_10Y_Min', 'Total_Costs_10Y_Max', 'Return_1Y_Moderate_Min', 'Return_1Y_Moderate_Max', 'Return_1Y_Moderate_Clean', 'Return_5Y_Moderate_Min', 'Return_5Y_Moderate_Max', 'Return_5Y_Moderate_Clean', 'Return_1Y_Favorable_Min', 'Return_1Y_Favorable_Max', 'Return_1Y_Favorable_Clean', 'Return_5Y_Favorable_Min', 'Return_5Y_Favorable_Max', 'Return_5Y_Favorable_Clean', 'Min_Premium_Min', 'Min_Premium_Max', 'Min_Premium_Clean', 'Flag_Prodotto_Complesso', 'Flag_Capital_Protection', 'Flag_Capital_Protection_Conditioned', 'Flag_Retail', 'Flag_Dati_Con_Range', 'Flag_ESG']\n",
            "\n",
            "=== ANALISI POST-UNIFORMAZIONE AVANZATA ===\n",
            "\n",
            "Product_Type:\n",
            "  Valori unici originali: 52\n",
            "  Valori unici uniformati: 9\n",
            "  Valori uniformati: ['Ramo I - Assicurazione Mista', 'Ramo I - Vita Intera', 'Ramo III - Unit Linked', 'Ramo I - Rivalutabile', 'Ramo I - Capitale Differito', 'Ramo I - Altre Forme', 'Ramo I - Temporanea Caso Morte', 'Ramo V - Capitalizzazione', 'Multiramo']\n",
            "\n",
            "Investment_Type:\n",
            "  Valori unici originali: 5\n",
            "  Valori unici uniformati: 5\n",
            "  Valori uniformati: ['Gestione Separata', 'Fondo Interno', 'Fondo Esterno', 'Non Applicabile', 'Gestione Separata + Fondo Interno']\n",
            "\n",
            "Risk_Category:\n",
            "  Valori unici originali: 2\n",
            "  Valori unici uniformati: 1\n",
            "  Valori uniformati: ['Basso (2-3)', nan]\n",
            "\n",
            "Capital_Protection:\n",
            "  Valori unici originali: 2\n",
            "  Valori unici uniformati: 1\n",
            "  Valori uniformati: ['Protezione Condizionata']\n",
            "\n",
            "Client_Type:\n",
            "  Valori unici originali: 3\n",
            "  Valori unici uniformati: 3\n",
            "  Valori uniformati: ['Clientela al Dettaglio', 'Retail e Professionale', 'Cliente Professionale']\n",
            "\n",
            "=== GESTIONE RANGE E VALORI CONDIZIONALI ===\n",
            "Risk_Level_Numeric: 10 valori con range su 81 totali\n",
            "Entry_Costs_Pct: 2 valori con range su 81 totali\n",
            "Management_Fees: 14 valori con range su 81 totali\n",
            "Total_Costs_1Y: 3 valori con range su 81 totali\n",
            "Total_Costs_5Y: 2 valori con range su 81 totali\n",
            "\n",
            "Tipologie Management Fees:\n",
            "  Fisso: 76\n",
            "  Performance-Based: 5\n",
            "\n",
            "Tipologie Capital Protection:\n",
            "  Protezione Condizionata: 81\n",
            "\n",
            "=== REPORT QUALIT√Ä DATI ===\n",
            "               Variabile  Completezza_%  Valori_Unici  Valori_Mancanti  Range_%  N_Range  N_Valori_Singoli\n",
            "      Product_Type_Clean          100.0           9.0              0.0      NaN      NaN               NaN\n",
            "   Investment_Type_Clean          100.0           5.0              0.0      NaN      NaN               NaN\n",
            "     Risk_Category_Clean          100.0           1.0              0.0      NaN      NaN               NaN\n",
            "Capital_Protection_Clean          100.0           1.0              0.0      NaN      NaN               NaN\n",
            "       Client_Type_Clean          100.0           3.0              0.0      NaN      NaN               NaN\n",
            "      Risk_Level_Numeric            NaN           NaN              NaN     12.3     10.0              71.0\n",
            "         Entry_Costs_Pct            NaN           NaN              NaN      2.5      2.0              79.0\n",
            "         Management_Fees            NaN           NaN              NaN     17.3     14.0              67.0\n",
            "          Total_Costs_1Y            NaN           NaN              NaN      3.7      3.0              78.0\n",
            "          Total_Costs_5Y            NaN           NaN              NaN      2.5      2.0              79.0\n",
            "         Total_Costs_10Y            NaN           NaN              NaN      0.0      0.0              81.0\n",
            "      Return_1Y_Moderate            NaN           NaN              NaN      1.2      1.0              80.0\n",
            "      Return_5Y_Moderate            NaN           NaN              NaN      1.2      1.0              80.0\n",
            "     Return_1Y_Favorable            NaN           NaN              NaN      1.2      1.0              80.0\n",
            "     Return_5Y_Favorable            NaN           NaN              NaN      1.2      1.0              80.0\n",
            "             Min_Premium            NaN           NaN              NaN      0.0      0.0              81.0\n",
            "\n",
            "=== STATISTICHE FINALI ===\n",
            "Dataset finale: 81 righe, 119 colonne\n",
            "Nuove colonne create: 79\n",
            "Tipologie di colonne aggiunte:\n",
            "  - Uniformazione categoriali: 22\n",
            "  - Gestione range (Min/Max/Avg): 33\n",
            "  - Flag di qualit√†: 11\n",
            "  - Flag derivati: 6\n",
            "  - Management fees dettagliati: 8\n",
            "\n",
            "Uniformazione avanzata completata con successo!\n",
            "Dataset pronto per analisi attuariali avanzate, pricing e modellazione del rischio.\n",
            "\n",
            "Funzionalit√† implementate:\n",
            "‚úì Gestione range e valori approssimativi\n",
            "‚úì Management fees condizionali e performance-based\n",
            "‚úì Capital protection dettagliata con tutte le casistiche\n",
            "‚úì Flag di qualit√† per identificare incertezze nei dati\n",
            "‚úì Variabili derivate per analisi di compliance e rischio\n",
            "‚úì Report automatico qualit√† dati\n"
          ]
        }
      ],
      "source": [
        "# Codice per l'uniformazione del dataset KID (Key Information Document) assicurativo\n",
        "# Versione avanzata con gestione di range, valori condizionali e formule complesse\n",
        "# Mantiene la terminologia tecnica assicurativa secondo gli standard normativi\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import Dict, List, Tuple, Union\n",
        "\n",
        "def uniforma_dataset_kid(df_concise: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Uniforma le variabili categoriali del dataset KID mantenendo\n",
        "    la terminologia tecnica assicurativa e gestendo range/valori condizionali\n",
        "    \"\"\"\n",
        "\n",
        "    # Copia del dataframe per evitare modifiche al dataset originale\n",
        "    df = df_concise.copy()\n",
        "\n",
        "    print(\"=== UNIFORMAZIONE DATASET KID ASSICURATIVO (VERSIONE AVANZATA) ===\")\n",
        "    print(f\"Dataset originale: {df.shape[0]} righe, {df.shape[1]} colonne\")\n",
        "\n",
        "    # 1. UNIFORMAZIONE PRODUCT_TYPE\n",
        "    print(\"\\n1. Uniformazione Product_Type...\")\n",
        "    df['Product_Type_Clean'] = df['Product_Type'].apply(uniforma_product_type)\n",
        "\n",
        "    # 2. UNIFORMAZIONE INVESTMENT_TYPE\n",
        "    print(\"2. Uniformazione Investment_Type...\")\n",
        "    df['Investment_Type_Clean'] = df['Investment_Type'].apply(uniforma_investment_type)\n",
        "\n",
        "    # 3. UNIFORMAZIONE RISK_CATEGORY\n",
        "    print(\"3. Uniformazione Risk_Category...\")\n",
        "    df['Risk_Category_Clean'] = df['Risk_Category'].apply(uniforma_risk_category)\n",
        "\n",
        "    # 4. UNIFORMAZIONE CAPITAL_PROTECTION\n",
        "    print(\"4. Uniformazione Capital_Protection...\")\n",
        "    df['Capital_Protection_Clean'] = df['Capital_Protection'].apply(uniforma_capital_protection)\n",
        "\n",
        "    # 5. UNIFORMAZIONE CLIENT_TYPE\n",
        "    print(\"5. Uniformazione Client_Type...\")\n",
        "    df['Client_Type_Clean'] = df['Client_Type'].apply(uniforma_client_type)\n",
        "\n",
        "    # 6. UNIFORMAZIONE CONTRACT_DURATION\n",
        "    print(\"6. Uniformazione Contract_Duration...\")\n",
        "    df['Contract_Duration_Clean'] = df['Contract_Duration'].apply(uniforma_contract_duration)\n",
        "\n",
        "    # 7. UNIFORMAZIONE TIME_HORIZON\n",
        "    print(\"7. Uniformazione Time_Horizon...\")\n",
        "    df['Time_Horizon_Clean'] = df['Time_Horizon'].apply(uniforma_time_horizon)\n",
        "\n",
        "    # 8. UNIFORMAZIONE SURRENDER_AVAILABLE\n",
        "    print(\"8. Uniformazione Surrender_Available...\")\n",
        "    df['Surrender_Available_Clean'] = df['Surrender_Available'].apply(uniforma_surrender_available)\n",
        "\n",
        "    # 9. PULIZIA AVANZATA VARIABILI NUMERICHE CON GESTIONE RANGE\n",
        "    print(\"9. Pulizia avanzata variabili numeriche con gestione range...\")\n",
        "    df = pulisci_variabili_numeriche_avanzate(df)\n",
        "\n",
        "    # 10. CREAZIONE VARIABILI DERIVATE\n",
        "    print(\"10. Creazione variabili derivate...\")\n",
        "    df = crea_variabili_derivate(df)\n",
        "\n",
        "    # 11. GESTIONE MANAGEMENT_FEES CONDIZIONALI\n",
        "    print(\"11. Gestione Management_Fees condizionali...\")\n",
        "    df = gestisci_management_fees_condizionali(df)\n",
        "\n",
        "    # Report finale\n",
        "    print(f\"\\nDataset finale: {df.shape[0]} righe, {df.shape[1]} colonne\")\n",
        "    print(\"Nuove colonne create:\", [col for col in df.columns if col.endswith('_Clean') or col.startswith('Flag_') or col.endswith('_Min') or col.endswith('_Max')])\n",
        "\n",
        "    return df\n",
        "\n",
        "def estrai_range_numerici(valore: str) -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Estrae valori min, max e medio da stringhe che contengono range\n",
        "    Restituisce (min_val, max_val, avg_val)\n",
        "    \"\"\"\n",
        "    if pd.isna(valore) or valore == \"\" or valore == \"N/A\":\n",
        "        return np.nan, np.nan, np.nan\n",
        "\n",
        "    valore_str = str(valore).replace(',', '.')\n",
        "\n",
        "    # Pattern per range con trattino: \"1,2% - 2,5%\", \"‚Ç¨1.000 - ‚Ç¨5.000\"\n",
        "    range_pattern1 = r'([\\d\\.,]+).*?[-‚Äì‚Äî]\\s*([\\d\\.,]+)'\n",
        "\n",
        "    # Pattern per range con \"da...a\": \"da 1% a 3%\", \"from 1000 to 5000\"\n",
        "    range_pattern2 = r'(?:da|from)\\s*([\\d\\.,]+).*?(?:a|to)\\s*([\\d\\.,]+)'\n",
        "\n",
        "    # Pattern per \"fino a\": \"fino a 2%\", \"up to 1000\"\n",
        "    fino_pattern = r'(?:fino a|up to|max|massimo)\\s*([\\d\\.,]+)'\n",
        "\n",
        "    # Pattern per \"almeno\": \"almeno 1000\", \"at least 500\"\n",
        "    almeno_pattern = r'(?:almeno|at least|min|minimo)\\s*([\\d\\.,]+)'\n",
        "\n",
        "    # Pattern per \"circa\": \"circa 1,5%\", \"approximately 1000\"\n",
        "    circa_pattern = r'(?:circa|approximately|~|¬±)\\s*([\\d\\.,]+)'\n",
        "\n",
        "    # Cerca range completo\n",
        "    match_range1 = re.search(range_pattern1, valore_str)\n",
        "    match_range2 = re.search(range_pattern2, valore_str)\n",
        "\n",
        "    if match_range1:\n",
        "        try:\n",
        "            min_val = float(re.sub(r'[^\\d\\.]', '', match_range1.group(1)))\n",
        "            max_val = float(re.sub(r'[^\\d\\.]', '', match_range1.group(2)))\n",
        "            avg_val = (min_val + max_val) / 2\n",
        "            return min_val, max_val, avg_val\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    if match_range2:\n",
        "        try:\n",
        "            min_val = float(re.sub(r'[^\\d\\.]', '', match_range2.group(1)))\n",
        "            max_val = float(re.sub(r'[^\\d\\.]', '', match_range2.group(2)))\n",
        "            avg_val = (min_val + max_val) / 2\n",
        "            return min_val, max_val, avg_val\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Cerca \"fino a\" (solo max)\n",
        "    match_fino = re.search(fino_pattern, valore_str)\n",
        "    if match_fino:\n",
        "        try:\n",
        "            max_val = float(re.sub(r'[^\\d\\.]', '', match_fino.group(1)))\n",
        "            return 0, max_val, max_val / 2\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Cerca \"almeno\" (solo min)\n",
        "    match_almeno = re.search(almeno_pattern, valore_str)\n",
        "    if match_almeno:\n",
        "        try:\n",
        "            min_val = float(re.sub(r'[^\\d\\.]', '', match_almeno.group(1)))\n",
        "            return min_val, np.nan, min_val\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Cerca \"circa\" (valore approssimativo)\n",
        "    match_circa = re.search(circa_pattern, valore_str)\n",
        "    if match_circa:\n",
        "        try:\n",
        "            val = float(re.sub(r'[^\\d\\.]', '', match_circa.group(1)))\n",
        "            return val * 0.9, val * 1.1, val  # ¬±10% di tolleranza\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Se non √® un range, prova a estrarre un singolo valore\n",
        "    numeri = re.findall(r'[\\d\\.,]+', valore_str.replace(',', '.'))\n",
        "    if numeri:\n",
        "        try:\n",
        "            val = float(re.sub(r'[^\\d\\.]', '', numeri[0]))\n",
        "            return val, val, val\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return np.nan, np.nan, np.nan\n",
        "\n",
        "def pulisci_variabili_numeriche_avanzate(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Pulisce e uniforma le variabili numeriche gestendo range, valori condizionali e formule\n",
        "    \"\"\"\n",
        "\n",
        "    # Variabili numeriche da processare\n",
        "    numeric_cols = [\n",
        "        'Risk_Level_Numeric', 'Entry_Costs_Pct', 'Management_Fees',\n",
        "        'Total_Costs_1Y', 'Total_Costs_5Y', 'Total_Costs_10Y',\n",
        "        'Return_1Y_Moderate', 'Return_5Y_Moderate',\n",
        "        'Return_1Y_Favorable', 'Return_5Y_Favorable',\n",
        "        'Min_Premium'\n",
        "    ]\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            print(f\"   Processando {col}...\")\n",
        "\n",
        "            # Estrai range per ogni valore\n",
        "            ranges = df[col].apply(estrai_range_numerici)\n",
        "\n",
        "            # Crea colonne separate per min, max e valore medio\n",
        "            df[f'{col}_Min'] = ranges.apply(lambda x: x[0] if isinstance(x, tuple) else np.nan)\n",
        "            df[f'{col}_Max'] = ranges.apply(lambda x: x[1] if isinstance(x, tuple) else np.nan)\n",
        "            df[f'{col}_Avg'] = ranges.apply(lambda x: x[2] if isinstance(x, tuple) else np.nan)\n",
        "\n",
        "            # Crea flag per indicare se √® un range\n",
        "            df[f'{col}_IsRange'] = ranges.apply(lambda x:\n",
        "                isinstance(x, tuple) and not pd.isna(x[0]) and not pd.isna(x[1]) and x[0] != x[1]\n",
        "            )\n",
        "\n",
        "            # Colonna principale con valore rappresentativo (media se range, valore singolo altrimenti)\n",
        "            df[f'{col}_Clean'] = df[f'{col}_Avg']\n",
        "\n",
        "    return df\n",
        "\n",
        "def gestisci_management_fees_condizionali(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Gestisce i Management_Fees con formule condizionali complesse\n",
        "    \"\"\"\n",
        "    if 'Management_Fees' in df.columns:\n",
        "\n",
        "        def estrai_fees_base_e_condizionali(fees_str):\n",
        "            if pd.isna(fees_str) or fees_str == \"\" or fees_str == \"N/A\":\n",
        "                return np.nan, np.nan, \"Fisso\"\n",
        "\n",
        "            fees_str = str(fees_str).lower()\n",
        "\n",
        "            # Pattern per fee base + maggiorazione condizionale\n",
        "            # Es: \"1,10% annuo sul rendimento della gestione separata; maggiorazione di 0,03% per ogni 0,1% di rendimento oltre il 3%\"\n",
        "            if 'maggiorazione' in fees_str or 'performance' in fees_str or 'oltre' in fees_str:\n",
        "                # Estrae fee base\n",
        "                base_match = re.search(r'([\\d,\\.]+)%', fees_str)\n",
        "                base_fee = float(base_match.group(1).replace(',', '.')) if base_match else np.nan\n",
        "\n",
        "                # Estrae maggiorazione\n",
        "                magg_match = re.search(r'maggiorazione.*?([\\d,\\.]+)%', fees_str)\n",
        "                extra_fee = float(magg_match.group(1).replace(',', '.')) if magg_match else np.nan\n",
        "\n",
        "                return base_fee, extra_fee, \"Performance-Based\"\n",
        "\n",
        "            # Pattern per fee variabile\n",
        "            elif 'variabile' in fees_str or 'variable' in fees_str:\n",
        "                min_val, max_val, avg_val = estrai_range_numerici(fees_str)\n",
        "                return avg_val, max_val - min_val if not pd.isna(max_val) and not pd.isna(min_val) else np.nan, \"Variabile\"\n",
        "\n",
        "            # Fee fisso\n",
        "            else:\n",
        "                min_val, max_val, avg_val = estrai_range_numerici(fees_str)\n",
        "                return avg_val, np.nan, \"Fisso\"\n",
        "\n",
        "        # Applica la funzione\n",
        "        fees_data = df['Management_Fees'].apply(estrai_fees_base_e_condizionali)\n",
        "\n",
        "        df['Management_Fees_Base'] = fees_data.apply(lambda x: x[0] if isinstance(x, tuple) else np.nan)\n",
        "        df['Management_Fees_Extra'] = fees_data.apply(lambda x: x[1] if isinstance(x, tuple) else np.nan)\n",
        "        df['Management_Fees_Type'] = fees_data.apply(lambda x: x[2] if isinstance(x, tuple) else \"Non Specificato\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def uniforma_product_type(product_type: str) -> str:\n",
        "    \"\"\"Uniforma la tipologia di prodotto secondo classificazione Solvency II\"\"\"\n",
        "    if pd.isna(product_type) or product_type == \"\":\n",
        "        return \"Non Specificato\"\n",
        "\n",
        "    product_type = str(product_type).lower()\n",
        "\n",
        "    # Ramo I - Assicurazioni sulla vita\n",
        "    if any(term in product_type for term in ['vita intera', 'vita_intera', 'whole life']):\n",
        "        return \"Ramo I - Vita Intera\"\n",
        "    elif any(term in product_type for term in ['mista', 'endowment']):\n",
        "        return \"Ramo I - Assicurazione Mista\"\n",
        "    elif any(term in product_type for term in ['rivalutabile', 'rivalutabil']):\n",
        "        return \"Ramo I - Rivalutabile\"\n",
        "    elif any(term in product_type for term in ['capitale differito', 'capital deferred']):\n",
        "        return \"Ramo I - Capitale Differito\"\n",
        "    elif any(term in product_type for term in ['temporanea', 'temporary']):\n",
        "        return \"Ramo I - Temporanea Caso Morte\"\n",
        "\n",
        "    # Ramo III - Unit Linked\n",
        "    elif any(term in product_type for term in ['unit linked', 'unit_linked', 'ramo iii', 'ramo_iii']):\n",
        "        return \"Ramo III - Unit Linked\"\n",
        "\n",
        "    # Ramo V - Capitalizzazione\n",
        "    elif any(term in product_type for term in ['capitalizzazione', 'capitalization']):\n",
        "        return \"Ramo V - Capitalizzazione\"\n",
        "\n",
        "    # Multiramo\n",
        "    elif any(term in product_type for term in ['multiramo', 'multi-ramo']):\n",
        "        return \"Multiramo\"\n",
        "\n",
        "    # Previdenza Complementare\n",
        "    elif any(term in product_type for term in ['previdenza', 'pensione', 'pension']):\n",
        "        return \"Previdenza Complementare\"\n",
        "\n",
        "    else:\n",
        "        return \"Ramo I - Altre Forme\"\n",
        "\n",
        "def uniforma_investment_type(investment_type: str) -> str:\n",
        "    \"\"\"Uniforma il tipo di investimento\"\"\"\n",
        "    if pd.isna(investment_type) or investment_type == \"\" or investment_type == \"N/A\":\n",
        "        return \"Non Applicabile\"\n",
        "\n",
        "    investment_type = str(investment_type).lower()\n",
        "\n",
        "    if 'gestione_separata' in investment_type and 'fondo_interno' in investment_type:\n",
        "        return \"Gestione Separata + Fondo Interno\"\n",
        "    elif 'gestione_separata' in investment_type or 'gestione separata' in investment_type:\n",
        "        return \"Gestione Separata\"\n",
        "    elif 'fondo_interno' in investment_type or 'fondo interno' in investment_type:\n",
        "        return \"Fondo Interno\"\n",
        "    elif 'fondo_esterno' in investment_type or 'fondo esterno' in investment_type:\n",
        "        return \"Fondo Esterno\"\n",
        "    elif 'etf' in investment_type:\n",
        "        return \"ETF\"\n",
        "    elif 'sicav' in investment_type:\n",
        "        return \"SICAV\"\n",
        "    else:\n",
        "        return \"Altro\"\n",
        "\n",
        "def uniforma_risk_category(risk_category: str) -> str:\n",
        "    \"\"\"Uniforma la categoria di rischio secondo scala 1-7 Solvency II\"\"\"\n",
        "    if pd.isna(risk_category) or risk_category == \"\":\n",
        "        return \"Non Classificato\"\n",
        "\n",
        "    risk_category = str(risk_category).lower()\n",
        "\n",
        "    if any(term in risk_category for term in ['molto basso', 'very low']):\n",
        "        return \"Molto Basso (1-2)\"\n",
        "    elif any(term in risk_category for term in ['basso', 'low']):\n",
        "        return \"Basso (2-3)\"\n",
        "    elif any(term in risk_category for term in ['medio-basso', 'medio basso', 'medium-low']):\n",
        "        return \"Medio-Basso (3-4)\"\n",
        "    elif any(term in risk_category for term in ['medio', 'medium']) and 'alto' not in risk_category:\n",
        "        return \"Medio (4-5)\"\n",
        "    elif any(term in risk_category for term in ['medio-alto', 'medio alto', 'medium-high']):\n",
        "        return \"Medio-Alto (5-6)\"\n",
        "    elif any(term in risk_category for term in ['alto', 'high']):\n",
        "        return \"Alto (6-7)\"\n",
        "    elif any(term in risk_category for term in ['molto alto', 'very high']):\n",
        "        return \"Molto Alto (7)\"\n",
        "    else:\n",
        "        return \"Non Classificato\"\n",
        "\n",
        "def uniforma_capital_protection(capital_protection: str) -> str:\n",
        "    \"\"\"Uniforma la protezione del capitale secondo terminologia MiFID II\"\"\"\n",
        "    if pd.isna(capital_protection) or capital_protection == \"\":\n",
        "        return \"Non Specificato\"\n",
        "\n",
        "    capital_protection = str(capital_protection).lower()\n",
        "\n",
        "    # Protezione totale\n",
        "    if capital_protection == \"si\" or capital_protection == \"s√¨\":\n",
        "        return \"Protezione Totale\"\n",
        "    elif capital_protection == \"no\":\n",
        "        return \"Nessuna Protezione\"\n",
        "\n",
        "    # Protezioni condizionate\n",
        "    elif 'solo decesso' in capital_protection or 'case of death' in capital_protection:\n",
        "        return \"Protezione Solo Caso Morte\"\n",
        "    elif 'finestre' in capital_protection or 'window' in capital_protection or 'specifiche' in capital_protection:\n",
        "        return \"Protezione a Finestre Temporali\"\n",
        "    elif 'scadenza' in capital_protection or 'maturity' in capital_protection:\n",
        "        return \"Protezione Solo a Scadenza\"\n",
        "    elif 'decesso' in capital_protection and ('riscatto' in capital_protection or 'finestre' in capital_protection):\n",
        "        return \"Protezione Mista (Decesso + Finestre)\"\n",
        "    elif '10 anni' in capital_protection or '10anni' in capital_protection:\n",
        "        return \"Protezione Dopo 10 Anni\"\n",
        "\n",
        "    # Protezioni parziali\n",
        "    elif 'parziale' in capital_protection:\n",
        "        return \"Protezione Parziale\"\n",
        "    elif any(perc in capital_protection for perc in ['80%', '90%', '95%', '100%']):\n",
        "        return \"Protezione Parziale Percentuale\"\n",
        "    elif 'almeno' in capital_protection and any(perc in capital_protection for perc in ['80', '90', '95']):\n",
        "        return \"Protezione Parziale Percentuale\"\n",
        "\n",
        "    # Protezioni con rischi\n",
        "    elif 'cambio' in capital_protection or 'currency' in capital_protection or 'usd' in capital_protection:\n",
        "        return \"Protezione con Rischio Cambio\"\n",
        "    elif 'penali' in capital_protection or 'penalty' in capital_protection:\n",
        "        return \"Protezione con Penalit√†\"\n",
        "\n",
        "    else:\n",
        "        return \"Protezione Condizionata\"\n",
        "\n",
        "def uniforma_client_type(client_type: str) -> str:\n",
        "    \"\"\"Uniforma il tipo di cliente secondo MiFID II\"\"\"\n",
        "    if pd.isna(client_type) or client_type == \"\":\n",
        "        return \"Non Specificato\"\n",
        "\n",
        "    client_type = str(client_type).lower()\n",
        "\n",
        "    if 'retail' in client_type and 'professionale' in client_type:\n",
        "        return \"Retail e Professionale\"\n",
        "    elif 'retail' in client_type:\n",
        "        return \"Clientela al Dettaglio\"\n",
        "    elif 'professionale' in client_type or 'professional' in client_type:\n",
        "        return \"Cliente Professionale\"\n",
        "    elif 'istituzionale' in client_type or 'institutional' in client_type:\n",
        "        return \"Cliente Istituzionale\"\n",
        "    elif 'controparte qualificata' in client_type or 'eligible counterparty' in client_type:\n",
        "        return \"Controparte Qualificata\"\n",
        "    else:\n",
        "        return \"Non Specificato\"\n",
        "\n",
        "def uniforma_contract_duration(duration: str) -> str:\n",
        "    \"\"\"Uniforma la durata contrattuale\"\"\"\n",
        "    if pd.isna(duration) or duration == \"\":\n",
        "        return \"Non Specificato\"\n",
        "\n",
        "    duration = str(duration).lower()\n",
        "\n",
        "    if 'vita intera' in duration or 'whole life' in duration:\n",
        "        return \"Vita Intera\"\n",
        "    elif 'anni' in duration:\n",
        "        # Estrae il numero di anni\n",
        "        anni = re.findall(r'\\d+', duration)\n",
        "        if anni:\n",
        "            n_anni = int(anni[0])\n",
        "            if n_anni <= 5:\n",
        "                return \"Breve Termine (‚â§5 anni)\"\n",
        "            elif n_anni <= 10:\n",
        "                return \"Medio Termine (6-10 anni)\"\n",
        "            elif n_anni <= 20:\n",
        "                return \"Lungo Termine (11-20 anni)\"\n",
        "            else:\n",
        "                return \"Lunghissimo Termine (>20 anni)\"\n",
        "    elif 'fino' in duration and 'decesso' in duration:\n",
        "        return \"Vita Intera\"\n",
        "    else:\n",
        "        return \"Durata Variabile\"\n",
        "\n",
        "def uniforma_time_horizon(time_horizon: str) -> str:\n",
        "    \"\"\"Uniforma l'orizzonte temporale raccomandato\"\"\"\n",
        "    if pd.isna(time_horizon) or time_horizon == \"\":\n",
        "        return \"Non Specificato\"\n",
        "\n",
        "    time_horizon = str(time_horizon).lower()\n",
        "\n",
        "    if any(term in time_horizon for term in ['breve', 'short', '1 anno', '2 anni']):\n",
        "        return \"Breve Termine\"\n",
        "    elif any(term in time_horizon for term in ['medio', 'medium']):\n",
        "        return \"Medio Termine\"\n",
        "    elif any(term in time_horizon for term in ['lungo', 'long']):\n",
        "        return \"Lungo Termine\"\n",
        "    elif 'almeno' in time_horizon:\n",
        "        # Estrae gli anni\n",
        "        anni = re.findall(r'\\d+', time_horizon)\n",
        "        if anni:\n",
        "            n_anni = int(anni[0])\n",
        "            if n_anni <= 3:\n",
        "                return \"Breve Termine\"\n",
        "            elif n_anni <= 7:\n",
        "                return \"Medio Termine\"\n",
        "            else:\n",
        "                return \"Lungo Termine\"\n",
        "    else:\n",
        "        return \"Non Specificato\"\n",
        "\n",
        "def uniforma_surrender_available(surrender: str) -> str:\n",
        "    \"\"\"Uniforma la disponibilit√† del riscatto\"\"\"\n",
        "    if pd.isna(surrender) or surrender == \"\":\n",
        "        return \"Non Specificato\"\n",
        "\n",
        "    surrender = str(surrender).lower()\n",
        "\n",
        "    if surrender == \"si\" or surrender == \"s√¨\":\n",
        "        return \"Riscatto Disponibile\"\n",
        "    elif surrender == \"no\":\n",
        "        return \"Riscatto Non Disponibile\"\n",
        "    elif 'limitato' in surrender or 'limited' in surrender:\n",
        "        return \"Riscatto Limitato\"\n",
        "    elif 'penalit√†' in surrender or 'penalty' in surrender:\n",
        "        return \"Riscatto con Penalit√†\"\n",
        "    else:\n",
        "        return \"Riscatto Condizionato\"\n",
        "\n",
        "def crea_variabili_derivate(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Crea variabili derivate per l'analisi attuariale\"\"\"\n",
        "\n",
        "    # Assicuriamo che Risk_Level_Numeric sia numerico\n",
        "    if 'Risk_Level_Numeric_Clean' in df.columns:\n",
        "        risk_numeric = df['Risk_Level_Numeric_Clean']\n",
        "    elif 'Risk_Level_Numeric' in df.columns:\n",
        "        risk_numeric = pd.to_numeric(df['Risk_Level_Numeric'], errors='coerce')\n",
        "    else:\n",
        "        risk_numeric = pd.Series([np.nan] * len(df))\n",
        "\n",
        "    # Flag per prodotti complessi secondo PRIIPs\n",
        "    df['Flag_Prodotto_Complesso'] = (\n",
        "        (df['Product_Type_Clean'].str.contains('Unit Linked|Multiramo', na=False)) |\n",
        "        (df['Investment_Type_Clean'].isin(['Fondo Esterno', 'ETF', 'SICAV'])) |\n",
        "        (risk_numeric >= 5)\n",
        "    ).astype(int)\n",
        "\n",
        "    # Flag per protezione del capitale\n",
        "    df['Flag_Capital_Protection'] = (\n",
        "        df['Capital_Protection_Clean'].isin([\n",
        "            'Protezione Totale', 'Protezione Parziale',\n",
        "            'Protezione Solo Caso Morte', 'Protezione Mista (Decesso + Finestre)',\n",
        "            'Protezione Parziale Percentuale'\n",
        "        ])\n",
        "    ).astype(int)\n",
        "\n",
        "    # Flag per protezione condizionata\n",
        "    df['Flag_Capital_Protection_Conditioned'] = (\n",
        "        df['Capital_Protection_Clean'].isin([\n",
        "            'Protezione a Finestre Temporali', 'Protezione Solo a Scadenza',\n",
        "            'Protezione Dopo 10 Anni', 'Protezione con Penalit√†'\n",
        "        ])\n",
        "    ).astype(int)\n",
        "\n",
        "    # Categoria di costo basata su Total_Costs_1Y_Clean\n",
        "    cost_col = None\n",
        "    for col in ['Total_Costs_1Y_Clean', 'Total_Costs_1Y_Avg', 'Total_Costs_1Y_Numeric_Clean']:\n",
        "        if col in df.columns:\n",
        "            cost_col = col\n",
        "            break\n",
        "\n",
        "    if cost_col and df[cost_col].notna().sum() > 0:\n",
        "        df['Categoria_Costo'] = pd.cut(\n",
        "            df[cost_col],\n",
        "            bins=[-np.inf, 1, 2.5, np.inf],\n",
        "            labels=['Basso', 'Medio', 'Alto']\n",
        "        )\n",
        "\n",
        "    # Flag per clientela retail\n",
        "    df['Flag_Retail'] = (\n",
        "        df['Client_Type_Clean'].str.contains('Retail', na=False)\n",
        "    ).astype(int)\n",
        "\n",
        "    # Flag per management fees variabili/performance-based\n",
        "    if 'Management_Fees_Type' in df.columns:\n",
        "        df['Flag_Fees_Variabili'] = (\n",
        "            df['Management_Fees_Type'].isin(['Performance-Based', 'Variabile'])\n",
        "        ).astype(int)\n",
        "\n",
        "    # Durata contratto in anni (estratta da Contract_Duration)\n",
        "    df['Durata_Anni'] = df['Contract_Duration'].astype(str)\\\n",
        "        .str.extract(r'(\\d+)').astype(float)\n",
        "\n",
        "    # Flag per range di valori (indica incertezza nei dati)\n",
        "    range_cols = [col for col in df.columns if col.endswith('_IsRange')]\n",
        "    if range_cols:\n",
        "        df['Flag_Dati_Con_Range'] = df[range_cols].any(axis=1).astype(int)\n",
        "\n",
        "    # Categoria di rischio semplificata\n",
        "    df['Risk_Category_Simple'] = risk_numeric.apply(categorizza_rischio_semplice)\n",
        "\n",
        "    # Flag per prodotti ESG/sostenibili (basato su nomi e descrizioni)\n",
        "    if 'Product_Name' in df.columns:\n",
        "        df['Flag_ESG'] = df['Product_Name'].str.contains(\n",
        "            'ESG|Sostenibil|Green|Climate|Environment|Social',\n",
        "            case=False, na=False\n",
        "        ).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "def categorizza_rischio_semplice(risk_level: float) -> str:\n",
        "    \"\"\"Categorizza il livello di rischio in modo semplificato\"\"\"\n",
        "    if pd.isna(risk_level):\n",
        "        return \"Non Classificato\"\n",
        "    elif risk_level <= 2:\n",
        "        return \"Basso Rischio\"\n",
        "    elif risk_level <= 4:\n",
        "        return \"Medio Rischio\"\n",
        "    elif risk_level <= 6:\n",
        "        return \"Alto Rischio\"\n",
        "    else:\n",
        "        return \"Rischio Molto Alto\"\n",
        "\n",
        "def analizza_uniformazione_avanzata(df_original: pd.DataFrame, df_clean: pd.DataFrame):\n",
        "    \"\"\"Analizza i risultati dell'uniformazione avanzata\"\"\"\n",
        "\n",
        "    print(\"\\n=== ANALISI POST-UNIFORMAZIONE AVANZATA ===\")\n",
        "\n",
        "    # Variabili categoriali principali\n",
        "    categorical_vars = [\n",
        "        ('Product_Type', 'Product_Type_Clean'),\n",
        "        ('Investment_Type', 'Investment_Type_Clean'),\n",
        "        ('Risk_Category', 'Risk_Category_Clean'),\n",
        "        ('Capital_Protection', 'Capital_Protection_Clean'),\n",
        "        ('Client_Type', 'Client_Type_Clean')\n",
        "    ]\n",
        "\n",
        "    for original, clean in categorical_vars:\n",
        "        if original in df_original.columns and clean in df_clean.columns:\n",
        "            print(f\"\\n{original}:\")\n",
        "            print(f\"  Valori unici originali: {df_original[original].nunique()}\")\n",
        "            print(f\"  Valori unici uniformati: {df_clean[clean].nunique()}\")\n",
        "            print(f\"  Valori uniformati: {list(df_clean[clean].unique())}\")\n",
        "\n",
        "    # Analisi range e valori condizionali\n",
        "    print(\"\\n=== GESTIONE RANGE E VALORI CONDIZIONALI ===\")\n",
        "\n",
        "    range_cols = [col for col in df_clean.columns if col.endswith('_IsRange')]\n",
        "    if range_cols:\n",
        "        for col in range_cols[:5]:  # Prime 5 colonne con range\n",
        "            base_col = col.replace('_IsRange', '')\n",
        "            n_ranges = df_clean[col].sum()\n",
        "            print(f\"{base_col}: {n_ranges} valori con range su {len(df_clean)} totali\")\n",
        "\n",
        "    # Analisi Management Fees condizionali\n",
        "    if 'Management_Fees_Type' in df_clean.columns:\n",
        "        print(\"\\nTipologie Management Fees:\")\n",
        "        fees_types = df_clean['Management_Fees_Type'].value_counts()\n",
        "        for fee_type, count in fees_types.items():\n",
        "            print(f\"  {fee_type}: {count}\")\n",
        "\n",
        "    # Analisi Capital Protection dettagliata\n",
        "    if 'Capital_Protection_Clean' in df_clean.columns:\n",
        "        print(\"\\nTipologie Capital Protection:\")\n",
        "        protection_types = df_clean['Capital_Protection_Clean'].value_counts()\n",
        "        for prot_type, count in protection_types.items():\n",
        "            print(f\"  {prot_type}: {count}\")\n",
        "\n",
        "def genera_report_qualita_dati(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Genera un report sulla qualit√† dei dati uniformati\"\"\"\n",
        "\n",
        "    report_data = []\n",
        "\n",
        "    # Analisi completezza dati per variabili principali\n",
        "    main_vars = [\n",
        "        'Product_Type_Clean', 'Investment_Type_Clean', 'Risk_Category_Clean',\n",
        "        'Capital_Protection_Clean', 'Client_Type_Clean'\n",
        "    ]\n",
        "\n",
        "    for var in main_vars:\n",
        "        if var in df.columns:\n",
        "            completezza = (df[var] != 'Non Specificato').sum() / len(df) * 100\n",
        "            report_data.append({\n",
        "                'Variabile': var,\n",
        "                'Completezza_%': round(completezza, 1),\n",
        "                'Valori_Unici': df[var].nunique(),\n",
        "                'Valori_Mancanti': (df[var] == 'Non Specificato').sum()\n",
        "            })\n",
        "\n",
        "    # Analisi range nelle variabili numeriche\n",
        "    numeric_vars = [col for col in df.columns if col.endswith('_IsRange')]\n",
        "    for var in numeric_vars:\n",
        "        base_var = var.replace('_IsRange', '')\n",
        "        if var in df.columns:\n",
        "            n_ranges = df[var].sum()\n",
        "            perc_ranges = n_ranges / len(df) * 100\n",
        "            report_data.append({\n",
        "                'Variabile': base_var,\n",
        "                'Range_%': round(perc_ranges, 1),\n",
        "                'N_Range': n_ranges,\n",
        "                'N_Valori_Singoli': len(df) - n_ranges\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(report_data)\n",
        "\n",
        "# Esempio di utilizzo completo\n",
        "if __name__ == \"__main__\":\n",
        "    # Supponiamo che df_concise sia gi√† caricato\n",
        "    # df_concise = pd.read_excel('kid_concise_dataset.xlsx')\n",
        "\n",
        "    print(\"Inizio uniformazione avanzata dataset KID...\")\n",
        "\n",
        "    # Uniformazione del dataset\n",
        "    df_uniformato = uniforma_dataset_kid(df_concise)\n",
        "\n",
        "    # Analisi dei risultati\n",
        "    analizza_uniformazione_avanzata(df_concise, df_uniformato)\n",
        "\n",
        "    # Genera report qualit√† dati\n",
        "    report_qualita = genera_report_qualita_dati(df_uniformato)\n",
        "    print(\"\\n=== REPORT QUALIT√Ä DATI ===\")\n",
        "    print(report_qualita.to_string(index=False))\n",
        "\n",
        "    # Statistiche finali\n",
        "    print(f\"\\n=== STATISTICHE FINALI ===\")\n",
        "    print(f\"Dataset finale: {df_uniformato.shape[0]} righe, {df_uniformato.shape[1]} colonne\")\n",
        "\n",
        "    # Conta nuove colonne create\n",
        "    new_cols = [col for col in df_uniformato.columns if\n",
        "                col.endswith('_Clean') or col.endswith('_Min') or col.endswith('_Max') or\n",
        "                col.endswith('_Avg') or col.endswith('_IsRange') or col.startswith('Flag_') or\n",
        "                col.endswith('_Type') or col.endswith('_Base') or col.endswith('_Extra')]\n",
        "\n",
        "    print(f\"Nuove colonne create: {len(new_cols)}\")\n",
        "    print(\"Tipologie di colonne aggiunte:\")\n",
        "    print(f\"  - Uniformazione categoriali: {len([c for c in new_cols if c.endswith('_Clean')])}\")\n",
        "    print(f\"  - Gestione range (Min/Max/Avg): {len([c for c in new_cols if c.endswith(('_Min', '_Max', '_Avg'))])}\")\n",
        "    print(f\"  - Flag di qualit√†: {len([c for c in new_cols if c.endswith('_IsRange')])}\")\n",
        "    print(f\"  - Flag derivati: {len([c for c in new_cols if c.startswith('Flag_')])}\")\n",
        "    print(f\"  - Management fees dettagliati: {len([c for c in new_cols if 'Management_Fees' in c and c != 'Management_Fees'])}\")\n",
        "\n",
        "    # Salvataggio del dataset uniformato\n",
        "    # df_uniformato.to_excel('kid_dataset_uniformato_avanzato.xlsx', index=False)\n",
        "    # df_uniformato.to_csv('kid_dataset_uniformato_avanzato.csv', index=False)\n",
        "    # report_qualita.to_excel('report_qualita_dati_kid.xlsx', index=False)\n",
        "\n",
        "    print(\"\\nUniformazione avanzata completata con successo!\")\n",
        "    print(\"Dataset pronto per analisi attuariali avanzate, pricing e modellazione del rischio.\")\n",
        "    print(\"\\nFunzionalit√† implementate:\")\n",
        "    print(\"‚úì Gestione range e valori approssimativi\")\n",
        "    print(\"‚úì Management fees condizionali e performance-based\")\n",
        "    print(\"‚úì Capital protection dettagliata con tutte le casistiche\")\n",
        "    print(\"‚úì Flag di qualit√† per identificare incertezze nei dati\")\n",
        "    print(\"‚úì Variabili derivate per analisi di compliance e rischio\")\n",
        "    print(\"‚úì Report automatico qualit√† dati\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "LgXPQ23Ne3hu"
      },
      "outputs": [],
      "source": [
        "df_uniformato.to_excel('kid_concise_dataset_CLEAN.xlsx', sheet_name='KID Clean',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "Wr2aH8hOmHKs",
        "outputId": "7797e616-29d1-42bd-8975-76d90ee3260e"
      },
      "outputs": [],
      "source": [
        "# Carica il dataset pulito\n",
        "df_clean = pd.read_excel('kid_concise_dataset_CLEAN.xlsx', sheet_name='KID Clean')\n",
        "\n",
        "# Verifica la qualit√†\n",
        "print(f\"Dataset pulito: {df_clean.shape}\")\n",
        "print(f\"Colonne numeriche: {len(df_clean.select_dtypes(include=[np.number]).columns)}\")\n",
        "\n",
        "# Prime analisi\n",
        "df_clean['Risk_Level_Numeric_Clean'].value_counts()\n",
        "df_clean['Company'].value_counts()\n",
        "df_clean.groupby('Company')['Risk_Level_Numeric_Clean'].mean()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07248eed84c546eba4580d7bb609faef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0dd928cafc32440893062f889d50f2cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53818003388047b19bbb506d0d349426",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_cedf31be4c504451a5361b1d29eac982",
            "value": "100%"
          }
        },
        "3365b0f19e6c4bb196c79f45025300bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe319c449609441e9aaef2fde689e042",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_07248eed84c546eba4580d7bb609faef",
            "value": "‚Äá68/68‚Äá[15:17&lt;00:00,‚Äá13.17s/it]"
          }
        },
        "53818003388047b19bbb506d0d349426": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "560af61faa984785924e76bf84785788": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd1908ba58524c5684cc2d4baa31b97f",
            "max": 68,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c42a58e4a0b144e59cd7326dbd54e7df",
            "value": 68
          }
        },
        "beecbeddb8ad49fc86c2ba9fc14e9200": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c42a58e4a0b144e59cd7326dbd54e7df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cedf31be4c504451a5361b1d29eac982": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd1908ba58524c5684cc2d4baa31b97f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eff0100e90be46808eb6f65cf3c00f23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0dd928cafc32440893062f889d50f2cd",
              "IPY_MODEL_560af61faa984785924e76bf84785788",
              "IPY_MODEL_3365b0f19e6c4bb196c79f45025300bc"
            ],
            "layout": "IPY_MODEL_beecbeddb8ad49fc86c2ba9fc14e9200"
          }
        },
        "fe319c449609441e9aaef2fde689e042": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
